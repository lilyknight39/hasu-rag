"""
OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨

æä¾› /v1/chat/completions ç«¯ç‚¹ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”
é›†æˆ RAG ç³»ç»Ÿï¼Œå¹¶æä¾› API key é‰´æƒã€å¹¶å‘æ§åˆ¶å’Œé¢‘ç‡é™åˆ¶
"""

from fastapi import FastAPI, HTTPException, Header, Request, Depends
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Literal, Union
import time
import uuid
import logging
import subprocess
import sys
from pathlib import Path
from contextlib import asynccontextmanager

from auth import AuthManager
from rag_wrapper import get_rag_system

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# OpenAI å…¼å®¹æ•°æ®æ¨¡å‹
# ============================================================================

class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = Field(default="hasu-rag", description="æ¨¡å‹åç§°")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    stream: Optional[bool] = Field(default=False, description="æ˜¯å¦æµå¼å“åº”")
    # ä»¥ä¸‹å‚æ•°ä»…ä¸ºå…¼å®¹æ€§å­˜åœ¨ï¼Œå°†è¢«å¿½ç•¥
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    n: Optional[int] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    logit_bias: Optional[dict] = None
    user: Optional[str] = None
    tools: Optional[list] = None
    tool_choice: Optional[Union[str, dict]] = None

class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: dict

class ModelCard(BaseModel):
    id: str
    object: str = "model"
    created: int = Field(default_factory=lambda: int(time.time()))
    owned_by: str = "hasu-rag"

class ModelList(BaseModel):
    object: str = "list"
    data: List[ModelCard]

# ============================================================================
# FastAPI åº”ç”¨
# ============================================================================

# ä½¿ç”¨ lifespan ç®¡ç†åº”ç”¨ç”Ÿå‘½å‘¨æœŸ
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨å¯åŠ¨å’Œå…³é—­æ—¶çš„é’©å­"""
    logger.info("ğŸš€ å¯åŠ¨ OpenAI å…¼å®¹ API æœåŠ¡å™¨...")
    logger.info("ğŸ“¦ é¢„åŠ è½½ RAG ç³»ç»Ÿ...")
    
    # é¢„åŠ è½½ RAG ç³»ç»Ÿï¼ˆé¿å…ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶åˆå§‹åŒ–å¤ªæ…¢ï¼‰
    try:
        get_rag_system()
        logger.info("âœ… RAG ç³»ç»ŸåŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ RAG ç³»ç»ŸåŠ è½½å¤±è´¥: {e}")

    mcp_process = None
    if os.getenv("ONEBOT_MCP_AUTOSTART", "0") == "1":
        script_path = (Path(__file__).parent / "mcp_onebot_server.py").resolve()
        if script_path.exists():
            try:
                mcp_process = subprocess.Popen(
                    [sys.executable, str(script_path)],
                    env=os.environ.copy()
                )
                logger.info(f"âœ… MCP OneBot server started (pid={mcp_process.pid})")
            except Exception as e:
                logger.warning(f"âš ï¸ MCP OneBot server failed to start: {e}")
        else:
            logger.warning(f"âš ï¸ MCP OneBot server script not found: {script_path}")
    
    yield

    if mcp_process:
        mcp_process.terminate()
        try:
            mcp_process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            mcp_process.kill()
    
    logger.info("ğŸ›‘ å…³é—­ API æœåŠ¡å™¨...")

app = FastAPI(
    title="Hasu-RAG OpenAI Compatible API",
    description="OpenAI å…¼å®¹çš„ RAG API æœåŠ¡å™¨ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å“åº”",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.middleware("http")
async def log_requests(request: Request, call_next):
    logger.info(f"ğŸ•’ {request.method} {request.url.path}")
    response = await call_next(request)
    logger.info(f"ğŸ•’ Result: {response.status_code}")
    return response

import os
import re

# åˆå§‹åŒ–è®¤è¯ç®¡ç†å™¨ (auth.py å†…éƒ¨å·²é»˜è®¤å¤„ç†ç›¸å¯¹è·¯å¾„)
auth_manager = AuthManager()

# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def clean_query(text: str) -> str:
    """
    å‰”é™¤æœºå™¨äººæ¡†æ¶äº§ç”Ÿçš„å¤šä½™ä¿¡æ¯ï¼ˆå¦‚ QQ æ˜µç§°ã€AT æ ‡ç­¾ç­‰ï¼‰
    """
    # 1. å‰”é™¤ <at ... /> æ ‡ç­¾
    text = re.sub(r'<at\b[^>]*\/?>', '', text)
    
    # 2. å‰”é™¤å¼€å¤´çš„æ˜µç§°å‰ç¼€ï¼ˆæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡å†’å·ï¼‰ï¼Œä¾‹å¦‚ "æ˜µç§°ï¼š" æˆ– "æ˜µç§°: "
    # æ³¨æ„ï¼šåªåŒ¹é…å¼€å¤´çš„ä¸€æ®µï¼Œé¿å…è¯¯åˆ æ­£æ–‡å†…å®¹
    text = re.sub(r'^[^ï¼š:\n]+[ï¼š:]\s*', '', text)
    
    return text.strip()

def extract_user_message(messages: List[ChatMessage]) -> str:
    """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–ç”¨æˆ·é—®é¢˜"""
    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    for msg in reversed(messages):
        if msg.role == "user":
            return clean_query(msg.content)
    
    raise HTTPException(
        status_code=400,
        detail="No user message found in the conversation"
    )

def create_sse_chunk(content: str, finish_reason: Optional[str] = None) -> str:
    """åˆ›å»º SSE (Server-Sent Events) æ ¼å¼çš„æ•°æ®å—"""
    chunk = {
        "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": "hasu-rag",
        "choices": [{
            "index": 0,
            "delta": {"content": content} if content else {},
            "finish_reason": finish_reason
        }]
    }
    
    import json
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def sync_stream_generator(question: str, api_key: str):
    """åŒæ­¥æµå¼ç”Ÿæˆå™¨ (åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ)"""
    try:
        rag = get_rag_system()
        
        # ä½¿ç”¨ RAG ç³»ç»Ÿæµå¼ç”Ÿæˆç­”æ¡ˆ
        for chunk in rag.query(question):
            yield create_sse_chunk(chunk)
        
        # å‘é€ç»“æŸæ ‡è®°
        yield create_sse_chunk("", finish_reason="stop")
        yield "data: [DONE]\n\n"
    
    except Exception as e:
        logger.error(f"âŒ æµå¼ç”Ÿæˆé”™è¯¯: {e}")
        error_chunk = {
            "error": {
                "message": f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "type": "internal_error"
            }
        }
        import json
        yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
    
    finally:
        # é‡Šæ”¾è¯·æ±‚æ§½ä½
        auth_manager.release_request_slot(api_key)

def fake_stream_generator(message: str):
    """æ¨¡æ‹Ÿæµå¼ç”Ÿæˆå™¨ï¼Œç”¨äºè¿”å›å‹å¥½çš„é”™è¯¯æç¤º"""
    yield create_sse_chunk(message)
    yield create_sse_chunk("", finish_reason="stop")
    yield "data: [DONE]\n\n"

# ============================================================================
# API ç«¯ç‚¹
# ============================================================================

@app.get("/")
def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "message": "Hasu-RAG OpenAI Compatible API",
        "version": "1.0.0",
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "health": "/health"
        }
    }

@app.get("/health")
def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "timestamp": int(time.time())}

@app.get("/v1/models")
def list_models():
    """
    åˆ—å‡ºå¯ç”¨æ¨¡å‹ (OpenAI å…¼å®¹)
    """
    logger.info("ğŸ” Received request for /v1/models")
    return ModelList(data=[ModelCard(id="hasu-rag")])

@app.post("/v1/chat/completions")
def chat_completions(
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None)
):
    """
    OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹
    
    æ”¯æŒæµå¼å’Œéæµå¼å“åº”
    """
    # 1. éªŒè¯å¹¶è·å–è¯·æ±‚æ§½ä½ï¼ˆåŒ…æ‹¬ API key éªŒè¯ã€å¹¶å‘æ£€æŸ¥ã€é¢‘ç‡é™åˆ¶ï¼‰
    try:
        api_key, config = auth_manager.acquire_request_slot(authorization)
    except HTTPException as e:
        # å°† 429 é”™è¯¯ä¼ªè£…æˆå‹å¥½çš„æœºå™¨äººå›å¤
        if e.status_code == 429 and isinstance(e.detail, dict):
            error_info = e.detail.get("error", {})
            raw_msg = error_info.get("message", "ç³»ç»Ÿç¹å¿™")
            friendly_msg = f"ã€ç³»ç»Ÿæé†’ã€‘{raw_msg}"
            
            if request.stream:
                return StreamingResponse(
                    fake_stream_generator(friendly_msg),
                    media_type="text/event-stream"
                )
            else:
                return ChatCompletionResponse(
                    id=f"chatcmpl-system-{uuid.uuid4().hex[:8]}",
                    created=int(time.time()),
                    model=request.model,
                    choices=[
                        ChatCompletionResponseChoice(
                            index=0,
                            message=ChatMessage(role="assistant", content=friendly_msg),
                            finish_reason="stop"
                        )
                    ],
                    usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                )
        raise
    
    # 2. æå–ç”¨æˆ·é—®é¢˜
    try:
        # æ‰“å°åŸå§‹è¯·æ±‚ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
        logger.info(f"ğŸ“¥ Received Chat Request: model={request.model}, stream={request.stream}")
        question = extract_user_message(request.messages)
        logger.info(f"ğŸ“ æœ€ç»ˆæå–é—®é¢˜ ({config.name}): {question}")
    except Exception as e:
        auth_manager.release_request_slot(api_key)
        raise HTTPException(status_code=400, detail=str(e))
    
    # 3. æ ¹æ® stream å‚æ•°é€‰æ‹©å“åº”æ¨¡å¼
    if request.stream:
        # æµå¼å“åº”
        logger.info("ğŸŒŠ ä½¿ç”¨æµå¼å“åº”")
        return StreamingResponse(
            sync_stream_generator(question, api_key),
            media_type="text/event-stream"
        )
    else:
        # éæµå¼å“åº”
        logger.info("ğŸ“„ ä½¿ç”¨éæµå¼å“åº”")
        try:
            rag = get_rag_system()
            
            # æ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„å†…å®¹
            full_response = ""
            for chunk in rag.query(question):
                full_response += chunk
            
            # æ„é€  OpenAI æ ¼å¼çš„å“åº”
            response = ChatCompletionResponse(
                id=f"chatcmpl-{uuid.uuid4().hex}",
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatCompletionResponseChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=full_response),
                        finish_reason="stop"
                    )
                ],
                usage={
                    "prompt_tokens": len(question),
                    "completion_tokens": len(full_response),
                    "total_tokens": len(question) + len(full_response)
                }
            )
            
            return response
        
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"ç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            )
        
        finally:
            # é‡Šæ”¾è¯·æ±‚æ§½ä½
            auth_manager.release_request_slot(api_key)

# ============================================================================
# é”™è¯¯å¤„ç†
# ============================================================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """è‡ªå®šä¹‰ HTTP å¼‚å¸¸å¤„ç†"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"error": {"message": exc.detail, "type": "invalid_request_error"}}
        if not isinstance(exc.detail, dict)
        else exc.detail
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    logger.error(f"âŒ æœªå¤„ç†çš„å¼‚å¸¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "Internal server error",
                "type": "internal_error"
            }
        }
    )

# ============================================================================
# ä¸»å…¥å£
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
