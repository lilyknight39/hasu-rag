import json
import os
import uuid
import warnings
from typing import List, Tuple

import env_loader  # load .env if present

# å±è”½è­¦å‘Š
warnings.filterwarnings("ignore")

# æ ¸å¿ƒç»„ä»¶
from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
from qdrant_client import QdrantClient
from langchain_community.embeddings import XinferenceEmbeddings
from langchain_core.documents import Document

# --- é…ç½® (å¿…é¡»ä¸åŸ ingest.py ä¿æŒå®Œå…¨ä¸€è‡´) ---
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
EMBED_MODEL = os.getenv("EMBEDDING_MODEL_UID", "bge-m3")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "story_knowledge_base")
SPARSE_VECTOR_NAME = "langchain-sparse" # å¿…é¡»ä¸å»ºè¡¨æ—¶ä¸€è‡´

def resolve_default_append_path() -> str:
    """
    é€‰æ‹©ä¸€ä¸ªå®é™…å­˜åœ¨çš„é»˜è®¤æ•°æ®è·¯å¾„ï¼Œé¿å…ç”¨æˆ·ç›´æ¥å›è½¦åæŒ‡å‘ä¸å­˜åœ¨çš„æ–‡ä»¶ã€‚
    ä»…æ”¯æŒ timeline_flow_optimized.json æ–°æ ¼å¼ã€‚
    """
    candidates = [
        os.getenv("APPEND_DATA_FILE", "").strip(),
        "/data/timeline_flow_optimized.json",
        "data/timeline_flow_optimized.json",
    ]
    for path in candidates:
        if path and os.path.exists(path):
            return path
    raise FileNotFoundError("æœªæ‰¾åˆ°å¯ç”¨çš„å¢é‡æ•°æ®æ–‡ä»¶ï¼Œè¯·è®¾ç½® APPEND_DATA_FILEã€‚")

def _normalize_text(item: dict) -> str:
    """ä»…æ”¯æŒ timeline_flow_optimized.json çš„ text/scriptã€‚"""
    text = item.get("text")
    if isinstance(text, str) and text.strip():
        return text
    script = item.get("script", [])
    if isinstance(script, list) and script:
        lines = []
        for turn in script:
            speaker = turn.get("c")
            text = turn.get("t", "")
            prefix = f"{speaker}: " if speaker else ""
            lines.append(f"{prefix}{text}")
        return "\n".join(lines)
    raise ValueError("æ–°æ ¼å¼æ•°æ®ç¼ºå°‘ text æˆ– script")


def _collect_strings(value) -> List[str]:
    if value is None:
        return []
    if isinstance(value, str):
        return [value]
    if isinstance(value, list):
        out = []
        for item in value:
            out.extend(_collect_strings(item))
        return out
    if isinstance(value, dict):
        out = []
        for item in value.values():
            out.extend(_collect_strings(item))
        return out
    return []


def _extract_meta_tokens(item: dict) -> List[str]:
    ctx = item.get("ctx") or {}
    tokens = []
    tokens.extend(_collect_strings(ctx.get("chars")))
    tokens.extend(_collect_strings(ctx.get("loc")))
    tokens.extend(_collect_strings(ctx.get("time")))
    tokens.extend(_collect_strings(ctx.get("emo")))
    tokens.extend(_collect_strings(ctx.get("state_emo")))
    seen = set()
    deduped = []
    for tok in tokens:
        tok = tok.strip()
        if not tok or tok in seen:
            continue
        seen.add(tok)
        deduped.append(tok)
    return deduped[:120]


def load_data_with_ids(file_path: str, order_offset: int = 0) -> Tuple[List[Document], List[str]]:
    """
    ä»…æ”¯æŒ timeline_flow_optimized.jsonï¼Œæ–°æ ¼å¼ç¼ºå¤±å…³é”®å­—æ®µå°†ç›´æ¥æŠ¥é”™ã€‚
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    if isinstance(data, dict):
        data = [data]
    if not isinstance(data, list):
        raise ValueError("æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œéœ€ä¸ºåˆ—è¡¨æˆ–å•æ¡å¯¹è±¡ã€‚")
    
    docs = []
    ids = []
    
    print(f"ğŸ“Š æ­£åœ¨è§£æ {len(data)} æ¡æ–°æ•°æ®...")

    for order_idx, item in enumerate(data):
        ctx = item.get("ctx") or {}
        stats = item.get("stats") or {}
        timeline = item.get("timeline") or {}

        raw_id = item.get("id") or item.get("scene")
        if raw_id:
            # åªè¦ ID ç›¸åŒï¼Œç”Ÿæˆçš„ UUID å°±ç›¸åŒ -> è¦†ç›–æ—§æ•°æ®
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, str(raw_id)))
        else:
            point_id = str(uuid.uuid4())
            raw_id = ""

        processed_meta = {
            "scene": item.get("scene") or raw_id or "unknown",
            "id": raw_id,
            "source": item.get("src", ""),
            "order": order_offset + order_idx,
            "chars": ctx.get("chars") or [],
            "voices": ctx.get("voices") or [],
            "loc": ctx.get("loc"),
            "time": ctx.get("time"),
            "bgm": ctx.get("bgm", ""),
            "type": ctx.get("type", ""),
            "stats": stats,
            "timeline": timeline,
            "act": ctx.get("act") or {},
            "emo": ctx.get("emo") or {},
            "state_act": ctx.get("state_act") or {},
            "state_emo": ctx.get("state_emo") or {},
            "state": ctx.get("state"),
            "weather": ctx.get("weather"),
            "merged_from": item.get("merged_from") or [],
            "script": item.get("script", []),
        }

        content = _normalize_text(item)
        meta_tokens = _extract_meta_tokens(item)
        if meta_tokens:
            content = f"{content}\n\n[meta] " + " ".join(meta_tokens)
        ids.append(point_id)
        docs.append(Document(page_content=content, metadata=processed_meta))
        
    return docs, ids

def main():
    print(f"\nğŸš€ å¯åŠ¨å¢é‡å…¥åº“æ¨¡å¼ (Incremental Ingestion)...")
    print(f"ğŸ¯ ç›®æ ‡é›†åˆ: {COLLECTION_NAME}")

    client = QdrantClient(url=QDRANT_URL)
    
    # 1. å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿é›†åˆå­˜åœ¨
    if not client.collection_exists(COLLECTION_NAME):
        print(f"âŒ é”™è¯¯: é›†åˆ '{COLLECTION_NAME}' ä¸å­˜åœ¨ï¼")
        print("   è¯·å…ˆè¿è¡Œ ingest.py è¿›è¡Œåˆå§‹åŒ–å»ºè¡¨ã€‚")
        return

    # 2. è¾“å…¥æ–°æ•°æ®è·¯å¾„
    try:
        default_path = resolve_default_append_path()
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        return
    file_path = input(f"ğŸ“‚ è¯·è¾“å…¥æ–°æ•°æ®æ–‡ä»¶è·¯å¾„ [é»˜è®¤: {default_path}]: ").strip()
    if not file_path:
        file_path = default_path
    
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
        print("   è¯·ç¡®è®¤è·¯å¾„ï¼Œæˆ–è®¾ç½® APPEND_DATA_FILE æŒ‡å‘æ­£ç¡®çš„æ•°æ®æ–‡ä»¶ã€‚")
        return

    # 3. åˆå§‹åŒ–æ¨¡å‹ (Dense + Sparse)
    print("ğŸ”Œ åˆå§‹åŒ– Embeddings (Xinference + FastEmbed)...")
    dense_embeddings = XinferenceEmbeddings(server_url=XINFERENCE_URL, model_uid=EMBED_MODEL)
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")

    # 4. åŠ è½½æ–°æ•°æ®
    try:
        existing_count = client.count(collection_name=COLLECTION_NAME, exact=True).count
        docs, ids = load_data_with_ids(file_path, order_offset=existing_count)
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return

    # 5. è¿æ¥ VectorStore (æ³¨æ„ï¼šè¿™é‡Œä¸åˆ›å»ºé›†åˆï¼Œåªè¿æ¥)
    vector_store = QdrantVectorStore(
        client=client, 
        collection_name=COLLECTION_NAME,
        embedding=dense_embeddings,
        sparse_embedding=sparse_embeddings,
        sparse_vector_name=SPARSE_VECTOR_NAME,
        retrieval_mode=RetrievalMode.HYBRID
    )
    
    print(f"ğŸŒŠ æ­£åœ¨è¿½åŠ /æ›´æ–° {len(docs)} æ¡æ•°æ®åˆ° Qdrant...")
    
    # 6. æ‰§è¡Œè¿½åŠ  (Add Documents)
    # Qdrant é»˜è®¤è¡Œä¸ºï¼šå¦‚æœ ID å­˜åœ¨åˆ™ Updateï¼Œä¸å­˜åœ¨åˆ™ Insert
    vector_store.add_documents(documents=docs, ids=ids)
    
    print(f"âœ… å¢é‡å…¥åº“å®Œæˆï¼")
    print(f"ğŸ’¡ æç¤º: æ–°æ•°æ®å·²ç«‹å³å¯è¢«æ£€ç´¢ã€‚")

if __name__ == "__main__":
    main()
