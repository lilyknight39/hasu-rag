import json
import os
import uuid
import warnings
from typing import List, Tuple

import env_loader  # load .env if present

# å±è”½æ¼äººçš„è­¦å‘Š
warnings.filterwarnings("ignore")

from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
from qdrant_client import QdrantClient
# [New] å¼•å…¥ç¨€ç–å‘é‡é…ç½®ç±»
from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams, SparseIndexParams
from langchain_community.embeddings import XinferenceEmbeddings
from langchain_core.documents import Document

# é…ç½®
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
EMBED_MODEL = os.getenv("EMBEDDING_MODEL_UID", "bge-m3")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "story_knowledge_base")

# å®šä¹‰ç¨€ç–å‘é‡çš„é»˜è®¤åç§° (LangChain é»˜è®¤ä½¿ç”¨è¿™ä¸ªåå­—)
SPARSE_VECTOR_NAME = "langchain-sparse"

def resolve_data_file() -> str:
    """
    å°è¯•è§£ææ•°æ®æ–‡ä»¶ä½ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨å®¹å™¨ä¸æœ¬åœ°çš„é»˜è®¤è·¯å¾„ã€‚
    è¿™æ ·æ— è®ºåœ¨ Dockerï¼ˆ/dataï¼‰è¿˜æ˜¯æœ¬åœ°ç›´æ¥è¿è¡Œï¼ˆdata/ï¼‰éƒ½èƒ½æ‰¾åˆ°æ–‡ä»¶ã€‚
    """
    candidates = [
        os.getenv("DATA_FILE", "").strip(),
        "/data/timeline_flow_optimized.json",
        "data/timeline_flow_optimized.json",
    ]
    candidates = [p for p in candidates if p]
    for path in candidates:
        if os.path.exists(path):
            return path
    raise FileNotFoundError(f"æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å­˜åœ¨: {', '.join(candidates)}")

def _normalize_text(item: dict) -> str:
    """ä»…æ”¯æŒ timeline_flow_optimized.json çš„ text/scriptã€‚"""
    text = item.get("text")
    if isinstance(text, str) and text.strip():
        return text
    script = item.get("script", [])
    if isinstance(script, list) and script:
        lines = []
        for turn in script:
            speaker = turn.get("c")
            text = turn.get("t", "")
            prefix = f"{speaker}: " if speaker else ""
            lines.append(f"{prefix}{text}")
        return "\n".join(lines)
    raise ValueError("æ–°æ ¼å¼æ•°æ®ç¼ºå°‘ text æˆ– script")


def _collect_strings(value) -> List[str]:
    if value is None:
        return []
    if isinstance(value, str):
        return [value]
    if isinstance(value, list):
        out = []
        for item in value:
            out.extend(_collect_strings(item))
        return out
    if isinstance(value, dict):
        out = []
        for item in value.values():
            out.extend(_collect_strings(item))
        return out
    return []


def _extract_meta_tokens(item: dict) -> List[str]:
    ctx = item.get("ctx") or {}
    tokens = []
    tokens.extend(_collect_strings(ctx.get("chars")))
    tokens.extend(_collect_strings(ctx.get("loc")))
    tokens.extend(_collect_strings(ctx.get("time")))
    tokens.extend(_collect_strings(ctx.get("emo")))
    tokens.extend(_collect_strings(ctx.get("state_emo")))
    # å»é‡å¹¶è£å‰ªï¼Œé¿å…å…ƒä¿¡æ¯è¿‡é•¿æ±¡æŸ“æ­£æ–‡
    seen = set()
    deduped = []
    for tok in tokens:
        tok = tok.strip()
        if not tok or tok in seen:
            continue
        seen.add(tok)
        deduped.append(tok)
    return deduped[:120]


def load_data_with_ids(file_path: str) -> Tuple[List[Document], List[str]]:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    if isinstance(data, dict):
        data = [data]
    if not isinstance(data, list):
        raise ValueError("æ•°æ®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œéœ€ä¸ºåˆ—è¡¨æˆ–å•æ¡å¯¹è±¡ã€‚")
    
    docs = []
    ids = []
    
    print(f"ğŸ“Š æ­£åœ¨è§£æ {len(data)} æ¡æ•°æ®...")

    for order_idx, item in enumerate(data):
        ctx = item.get("ctx") or {}
        stats = item.get("stats") or {}
        timeline = item.get("timeline") or {}

        raw_id = item.get("id") or item.get("scene")
        if raw_id:
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, str(raw_id)))
        else:
            point_id = str(uuid.uuid4())
            raw_id = ""

        processed_meta = {
            "scene": item.get("scene") or raw_id or "unknown",
            "id": raw_id,
            "source": item.get("src", ""),
            "order": order_idx,  # ä¿ç•™åŸå§‹æ—¶é—´é¡ºåº
            "chars": ctx.get("chars") or [],
            "voices": ctx.get("voices") or [],
            "loc": ctx.get("loc"),
            "time": ctx.get("time"),
            "bgm": ctx.get("bgm", ""),
            "type": ctx.get("type", ""),
            "stats": stats,
            "timeline": timeline,
            "act": ctx.get("act") or {},
            "emo": ctx.get("emo") or {},
            "state_act": ctx.get("state_act") or {},
            "state_emo": ctx.get("state_emo") or {},
            "state": ctx.get("state"),
            "weather": ctx.get("weather"),
            "merged_from": item.get("merged_from") or [],
            # ä¿ç•™åŸå§‹è„šæœ¬ä»¥é¿å…ä¿¡æ¯ä¸¢å¤±ï¼ˆå« t/c/v/kf ç­‰ï¼‰
            "script": item.get("script", []),
        }

        content = _normalize_text(item)
        meta_tokens = _extract_meta_tokens(item)
        if meta_tokens:
            content = f"{content}\n\n[meta] " + " ".join(meta_tokens)
        ids.append(point_id)
        docs.append(Document(page_content=content, metadata=processed_meta))
        
    return docs, ids

def main():
    print("ğŸš€ å¼€å§‹ v1.x æ··åˆæ£€ç´¢å…¥åº“ (Hybrid Ingestion)...")

    client = QdrantClient(url=QDRANT_URL)
    
    # 1. å½»åº•é‡å»ºé›†åˆ (æ‰‹åŠ¨å®šä¹‰ Hybrid Schema)
    if client.collection_exists(COLLECTION_NAME):
        print(f"ğŸ—‘ï¸ æ­£åœ¨æ¸…ç†æ—§é›†åˆ...")
        client.delete_collection(COLLECTION_NAME)

    print(f"ğŸ› ï¸ æ­£åœ¨åˆ›å»ºæ··åˆæ£€ç´¢é›†åˆ: {COLLECTION_NAME}")
    # [å…³é”®ä¿®å¤] åœ¨åˆå§‹åŒ– VectorStore å‰ï¼Œæ‰‹åŠ¨åˆ›å»ºå¥½é›†åˆç»“æ„
    client.create_collection(
        collection_name=COLLECTION_NAME,
        # A. å¯†é›†å‘é‡é…ç½® (Dense - BGE-M3)
        # ä½¿ç”¨ç©ºå­—ç¬¦ä¸² "" ä½œä¸ºé»˜è®¤å‘é‡åï¼Œè¿™æ˜¯ Qdrant æ ‡å‡†
        vectors_config={
            "": VectorParams(
                size=1024, 
                distance=Distance.COSINE
            )
        },
        # B. ç¨€ç–å‘é‡é…ç½® (Sparse - BM25)
        # å¿…é¡»æ˜¾å¼å®šä¹‰ sparse_vectors_config
        sparse_vectors_config={
            SPARSE_VECTOR_NAME: SparseVectorParams(
                index=SparseIndexParams(
                    on_disk=False, # æ”¾åœ¨å†…å­˜é‡Œæ›´å¿«
                )
            )
        }
    )

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ”Œ è¿æ¥ Xinference (Dense)...")
    dense_embeddings = XinferenceEmbeddings(
        server_url=XINFERENCE_URL, 
        model_uid=EMBED_MODEL
    )
    
    print("ğŸ”Œ åˆå§‹åŒ– FastEmbed (Sparse)...")
    sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")

    # 3. åŠ è½½æ•°æ®
    try:
        data_path = resolve_data_file()
        print(f"ğŸ“‚ æ•°æ®æ–‡ä»¶: {data_path}")
        docs, ids = load_data_with_ids(data_path)
    except Exception as e:
        print(f"âŒ è¯»å–æˆ–è§£ææ•°æ®å¤±è´¥: {e}")
        return

    print(f"ğŸ“„ å‡†å¤‡å†™å…¥ {len(docs)} æ¡æ•°æ®...")
    
    # 4. å®ä¾‹åŒ– VectorStore
    # ç°åœ¨é›†åˆå·²ç»å­˜åœ¨äº†ï¼Œæ ¡éªŒå¯ä»¥é€šè¿‡
    vector_store = QdrantVectorStore(
        client=client, 
        collection_name=COLLECTION_NAME,
        embedding=dense_embeddings,
        sparse_embedding=sparse_embeddings,
        sparse_vector_name=SPARSE_VECTOR_NAME, # æ˜¾å¼æŒ‡å®šç¨€ç–å‘é‡å
        retrieval_mode=RetrievalMode.HYBRID
    )
    
    print("ğŸŒŠ æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶ä¸Šä¼  (Dense + Sparse)...")
    vector_store.add_documents(documents=docs, ids=ids)
    
    print(f"âœ… æ··åˆæ£€ç´¢åº“æ„å»ºå®Œæˆï¼")

if __name__ == "__main__":
    main()
