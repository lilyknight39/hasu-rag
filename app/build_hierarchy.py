import os
import json
import time
import numpy as np
import warnings
from sklearn.cluster import KMeans
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import XinferenceEmbeddings
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph

# å±è”½è­¦å‘Š
warnings.filterwarnings("ignore")

# --- é…ç½® ---
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
SOURCE_COLLECTION = "story_knowledge_base" 
SUMMARY_COLLECTION = "story_summary_store" 

# LLM é…ç½®
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.example.com/v1")
LLM_API_KEY = os.getenv("LLM_API_KEY", "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "your-")

def fetch_all_vectors(client, collection_name):
    """ä» Qdrant æ‹‰å–æ‰€æœ‰å‘é‡"""
    print("ğŸ“¥ åŸå§‹ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ä¸­ (Fetching chunks)...")
    points = []
    offset = None
    while True:
        result = client.scroll(
            collection_name=collection_name,
            limit=100,
            with_payload=True,
            with_vectors=True,
            offset=offset
        )
        batch, next_offset = result
        points.extend(batch)
        if next_offset is None:
            break
        offset = next_offset
    print(f"âœ… åˆè¨ˆ {len(points)} å€‹ã®æ–­ç‰‡ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return points

def perform_clustering(points, n_clusters=None):
    """
    [Advanced] ä½¿ç”¨ 'å¸¦è¿æ¥æ€§çº¦æŸçš„å±‚æ¬¡èšç±»' (Agglomerative with Connectivity)
    ç»“åˆäº† K-Means çš„å…¨å±€ä¼˜åŒ–èƒ½åŠ›å’Œ Sequential çš„æ—¶é—´çº¦æŸã€‚
    """
    print(f"ğŸ§® å®Ÿè¡Œä¸­: æ™‚é–“åˆ¶ç´„ä»˜ãéšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (Constrained Agglomerative Clustering)...")
    
    if not points:
        return {}

    # 1. æå–å‘é‡ (å¤ç”¨ä¹‹å‰çš„å¥å£®é€»è¾‘)
    vectors_list = []
    for p in points:
        vec = p.vector
        if isinstance(vec, dict):
            dense_vec = vec.get("", vec.get("default"))
            if dense_vec is None:
                for v in vec.values():
                    if isinstance(v, list):
                        dense_vec = v
                        break
            vectors_list.append(dense_vec)
        elif isinstance(vec, list):
            vectors_list.append(vec)
    
    X = np.array(vectors_list)
    
    # 2. å…³é”®æ­¥éª¤ï¼šæ„å»ºè¿æ¥æ€§çŸ©é˜µ (Connectivity Matrix)
    # è¿™å‘Šè¯‰ç®—æ³•ï¼šX[i] åªèƒ½å’Œ X[i-1] æˆ– X[i+1] åˆå¹¶ï¼Œä¸èƒ½è·¨æ—¶é—´è·³è·ƒ
    # n_neighbors=1 è¡¨ç¤ºåªè¿æ¥æœ€è¿‘çš„ 1 ä¸ªé‚»å±…ï¼ˆå³æ—¶é—´ä¸Šçš„å‰ä¸€ä¸ª/åä¸€ä¸ªï¼‰
    connectivity = kneighbors_graph(X, n_neighbors=1, include_self=False)

    # 3. åŠ¨æ€ç¡®å®šç°‡çš„æ•°é‡
    # å¦‚æœæ²¡æœ‰æŒ‡å®š n_clustersï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºå‹ç¼©æ¯”è‡ªåŠ¨è®¡ç®—
    # æˆ–è€…ä½¿ç”¨ distance_threshold (è·ç¦»é˜ˆå€¼) è®©æ•°æ®è‡ªå·±å†³å®š
    if n_clusters is None:
        target_n_clusters = max(1, len(X) // 8) # é»˜è®¤æ¯8ä¸ªåˆå¹¶ä¸ºä¸€ä¸ª
    else:
        target_n_clusters = n_clusters

    print(f"   ğŸ¯ ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_n_clusters}")

    # 4. æ‰§è¡Œèšç±»
    # linkage='ward': æœ€å°åŒ–ç°‡å†…çš„æ–¹å·®ï¼ˆæœ€å¸¸ç”¨çš„åˆå¹¶ç­–ç•¥ï¼‰
    # connectivity=connectivity: åŠ ä¸Šæ—¶é—´é”é“¾
    model = AgglomerativeClustering(
        n_clusters=target_n_clusters, 
        connectivity=connectivity,
        linkage='ward' 
    )
    
    model.fit(X)
    
    # 5. æ•´ç†ç»“æœ
    clusters = {}
    for idx, label in enumerate(model.labels_):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(points[idx])
    
    # [é‡è¦ä¼˜åŒ–] å±‚æ¬¡èšç±»çš„ label ID æ˜¯ä¹±åºçš„ï¼Œæˆ‘ä»¬éœ€è¦æŒ‰æ—¶é—´é‡æ–°æ’åº Cluster ID
    # å¦åˆ™ç”Ÿæˆçš„æ‘˜è¦é¡ºåºä¼šä¹±
    sorted_cluster_ids = sorted(clusters.keys(), key=lambda k: clusters[k][0].id)
    
    ordered_clusters = {}
    for new_id, old_id in enumerate(sorted_cluster_ids):
        ordered_clusters[new_id] = clusters[old_id]
        
    print(f"âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {len(ordered_clusters)} å€‹ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚")
    return ordered_clusters

def generate_summary(llm, cluster_points):
    """
    [Upgrade] ä½¿ç”¨çº¯æ—¥æ–‡ Promptï¼Œå¹¶å¼ºåˆ¶æ ¼å¼åŒ–è¾“å‡º
    """
    texts = [p.payload['page_content'] for p in cluster_points]
    combined_text = "\n---\n".join(texts)[:15000]
    
    # çº¯æ—¥æ–‡ Promptï¼Œå¼ºè°ƒå‰§æƒ…æ¦‚æ‹¬å’Œæ ¼å¼æ§åˆ¶
    template = """ã‚ãªãŸã¯ã€è“®ãƒç©ºå¥³å­¦é™¢ã‚¹ã‚¯ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¯ãƒ©ãƒ–ã€ã®ã‚·ãƒŠãƒªã‚ªç·¨é›†ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸä¸€é€£ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ–­ç‰‡ï¼ˆåŒã˜æ™‚é–“å¸¯ã‚„ãƒ†ãƒ¼ãƒï¼‰ã§ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘:
ã“ã‚Œã‚‰ã®æ–­ç‰‡ã‚’èª­ã¿ã€**æ ¸å¿ƒçš„ãªå‡ºæ¥äº‹ã€ä¸»ãªå¯¾ç«‹ã€é–¢ä¸ã™ã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼**ã‚’è¦ç´„ã—ãŸã€ç°¡æ½”ãªã‚ã‚‰ã™ã˜ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€åˆ¶ç´„äº‹é …ï¼ˆå³å®ˆï¼‰ã€‘:
1. **å‡ºåŠ›ã¯è¦ç´„æœ¬æ–‡ã®ã¿**ã¨ã—ã¦ãã ã•ã„ã€‚ã€Œã¯ã„ã€ã€Œä»¥ä¸‹ã¯è¦ç´„ã§ã™ã€ãªã©ã®æŒ¨æ‹¶ã‚„å‰ç½®ãã¯**ä¸€åˆ‡ç¦æ­¢**ã§ã™ã€‚
2. æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
3. 200ã€œ400æ–‡å­—ç¨‹åº¦ã®ã€å®¢è¦³çš„ãªä¸‰äººç§°è¦–ç‚¹ã§æ›¸ã„ã¦ãã ã•ã„ã€‚
4. å…·ä½“çš„ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åï¼ˆèŠ±å¸†ã€æ¢¢ãªã©ï¼‰ã‚’æ­£ç¢ºã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ã€‘:
{context}

ã€è¦ç´„å‡ºåŠ›ã€‘:
"""
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    
    try:
        return chain.invoke({"context": combined_text}).strip()
    except Exception as e:
        print(f"âš ï¸ Summary Error: {e}")
        return "è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def main():
    client = QdrantClient(url=QDRANT_URL)
    
    # 1. è¯¢é—®æ¨¡å¼
    print("\n" + "="*50)
    print("ğŸ› ï¸  Build Hierarchy Index (Japanese Mode)")
    print("="*50)
    
    mode = input("è¯·é€‰æ‹©æ¨¡å¼ / ãƒ¢ãƒ¼ãƒ‰é¸æŠ:\n [1] æ–­ç‚¹ç»­ä¼  (Resume / æ¨å¥¨)\n [2] è¦†ç›–é‡å†™ (Overwrite / å…¨éƒ¨å‰Šé™¤ã—ã¦å†ä½œæˆ)\nè¾“å…¥ 1 or 2 [é»˜è®¤ 1]: ").strip()
    
    is_overwrite = (mode == '2')
    
    # 2. æ‹‰å–æ•°æ®
    all_points = fetch_all_vectors(client, SOURCE_COLLECTION)
    if not all_points: return
    
    # æŒ‰ ID æ’åºç¡®ä¿ K-Means ç»“æœä¸€è‡´æ€§
    all_points.sort(key=lambda p: p.id)

    # 3. èšç±»
    n_clusters = max(1, len(all_points) // 8)
    clusters = perform_clustering(all_points, n_clusters)
    
    # 4. é›†åˆç®¡ç†
    if is_overwrite:
        if client.collection_exists(SUMMARY_COLLECTION):
            print(f"ğŸ—‘ï¸  æ—§æ‘˜è¦é›†åˆå·²åˆ é™¤ (Deleting collection)...")
            client.delete_collection(SUMMARY_COLLECTION)
        print(f"ğŸ†• åˆ›å»ºæ–°é›†åˆ (Creating new collection)...")
        client.create_collection(
            collection_name=SUMMARY_COLLECTION,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
    else:
        # Resume æ¨¡å¼ï¼šå¦‚æœé›†åˆä¸å­˜åœ¨ï¼Œä¹Ÿå¾—åˆ›å»º
        if not client.collection_exists(SUMMARY_COLLECTION):
            print(f"ğŸ†• é›†åˆä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º (Creating collection)...")
            client.create_collection(
                collection_name=SUMMARY_COLLECTION,
                vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
            )
        else:
            print(f"ğŸ”„ æ­£åœ¨æ–­ç‚¹ç»­ä¼  (Resuming)...")

    # åˆå§‹åŒ–æ¨¡å‹
    llm = ChatOpenAI(
        base_url=LLM_BASE_URL,
        api_key=LLM_API_KEY,
        model=LLM_MODEL_NAME,
        temperature=0.1 # æ‘˜è¦ç”Ÿæˆéœ€è¦ä½æ¸©åº¦
    )
    embed_model = XinferenceEmbeddings(server_url=XINFERENCE_URL, model_uid="bge-m3")

    print("\nğŸ“ è¦ç´„ç”Ÿæˆã¨å…¥åº«ã‚’é–‹å§‹ã—ã¾ã™ (Generating summaries)...")
    
    existing_count = client.count(SUMMARY_COLLECTION).count
    print(f"   ç¾åœ¨ã®é€²æ— (Current count): {existing_count}/{n_clusters}")

    for cluster_id, points in clusters.items():
        # å¦‚æœä¸æ˜¯è¦†ç›–æ¨¡å¼ï¼Œä¸” ID å·²å­˜åœ¨ï¼Œåˆ™è·³è¿‡
        if not is_overwrite:
            check = client.retrieve(collection_name=SUMMARY_COLLECTION, ids=[cluster_id])
            if check:
                print(f"   â© Cluster {cluster_id+1}/{n_clusters} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ (Skipping).")
                continue

        print(f"   â–¶ï¸ Processing Cluster {cluster_id+1}/{n_clusters} (Fragments: {len(points)})...")
        
        try:
            # A. ç”Ÿæˆæ—¥æ–‡æ‘˜è¦
            summary_text = generate_summary(llm, points)
            
            # B. è®¡ç®—å‘é‡
            vector = embed_model.embed_query(summary_text)
            
            # C. ä¸Šä¼ 
            child_ids = [p.id for p in points]
            point = PointStruct(
                id=cluster_id, 
                vector=vector,
                payload={
                    "page_content": summary_text,
                    "child_ids": child_ids,
                    "level": "summary",
                    "count": len(points)
                }
            )
            client.upsert(collection_name=SUMMARY_COLLECTION, points=[point])
            
            # è¿™é‡Œçš„æ‘˜è¦æ‰“å°å‡ºæ¥çœ‹çœ‹ï¼Œç¡®è®¤æ˜¯å¦çº¯å‡€
            print(f"      [æ‘˜è¦é¢„è§ˆ]: {summary_text[:50]}...")
            
        except Exception as e:
            print(f"âŒ Error processing cluster {cluster_id}: {e}")
            time.sleep(2)

    print(f"\nğŸ‰ æ§‹ç¯‰å®Œäº†ï¼ (Build Complete)")

if __name__ == "__main__":
    main()