import os
import json
import time
import numpy as np
import warnings
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import kneighbors_graph
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import XinferenceEmbeddings

# å±è”½è­¦å‘Š
warnings.filterwarnings("ignore")

# --- é…ç½® ---
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
SOURCE_COLLECTION = "story_knowledge_base" 
SUMMARY_COLLECTION = "story_summary_store" 

# LLM é…ç½®
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "")
LLM_API_KEY = os.getenv("LLM_API_KEY", "")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "")

def fetch_all_vectors(client, collection_name):
    """ä» Qdrant æ‹‰å–æ‰€æœ‰å‘é‡"""
    print("ğŸ“¥ åŸå§‹ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ä¸­ (Fetching chunks)...")
    points = []
    offset = None
    while True:
        result = client.scroll(
            collection_name=collection_name,
            limit=100,
            with_payload=True,
            with_vectors=True,
            offset=offset
        )
        batch, next_offset = result
        points.extend(batch)
        if next_offset is None:
            break
        offset = next_offset
    
    # [å…³é”®] æŒ‰æ—¶é—´é¡ºåºæ’åºï¼šä¼˜å…ˆä½¿ç”¨ metadata.orderï¼Œå…¶æ¬¡ scene/id
    def sort_key(p):
        payload = p.payload or {}
        meta = payload.get("metadata") if isinstance(payload.get("metadata"), dict) else payload
        order = meta.get("order")
        if order is not None:
            try:
                return (0, int(order))
            except Exception:
                return (0, str(order))
        scene = meta.get("scene") or meta.get("scene_id") or meta.get("id")
        if scene:
            return (1, str(scene))
        return (2, str(p.id))

    points.sort(key=sort_key)
    
    print(f"âœ… åˆè¨ˆ {len(points)} å€‹ã®æ–­ç‰‡ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return points

def perform_clustering(points, n_clusters=None):
    """ä½¿ç”¨å¸¦æ—¶é—´çº¦æŸçš„å±‚æ¬¡èšç±»"""
    print(f"ğŸ§® æ™‚é–“åˆ¶ç´„ä»˜ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­ (Time-Constrained Agglomerative)...")
    
    if not points: return {}

    # 1. æå–å‘é‡
    vectors_list = []
    for p in points:
        vec = p.vector
        if isinstance(vec, dict):
            dense_vec = vec.get("", vec.get("default"))
            if dense_vec is None:
                for v in vec.values():
                    if isinstance(v, list):
                        dense_vec = v
                        break
            vectors_list.append(dense_vec)
        elif isinstance(vec, list):
            vectors_list.append(vec)
    
    X = np.array(vectors_list)
    
    # 2. è¿æ¥æ€§çº¦æŸ (Time Constraints)
    connectivity = kneighbors_graph(X, n_neighbors=1, include_self=False)

    # 3. åŠ¨æ€ç¡®å®šç°‡æ•°
    if n_clusters is None:
        target_n_clusters = max(1, len(X) // 8)
    else:
        target_n_clusters = n_clusters
    
    print(f"   ğŸ¯ ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_n_clusters}")

    # 4. èšç±»
    model = AgglomerativeClustering(
        n_clusters=target_n_clusters, 
        connectivity=connectivity,
        linkage='ward' 
    )
    model.fit(X)
    
    # 5. æ•´ç†ç»“æœå¹¶æŒ‰æ—¶é—´é‡æ’åº
    raw_clusters = {}
    for idx, label in enumerate(model.labels_):
        if label not in raw_clusters: raw_clusters[label] = []
        raw_clusters[label].append(points[idx])
    
    # æ ¹æ®æ¯ä¸ªç°‡ä¸­ç¬¬ä¸€ä¸ªç‰‡æ®µçš„IDè¿›è¡Œæ’åºï¼Œä¿è¯ Cluster 0 æ˜¯æœ€æ—©çš„å‰§æƒ…
    sorted_keys = sorted(raw_clusters.keys(), key=lambda k: raw_clusters[k][0].id)
    
    ordered_clusters = {}
    for new_id, old_id in enumerate(sorted_keys):
        ordered_clusters[new_id] = raw_clusters[old_id]
        
    return ordered_clusters

def _strip_meta_suffix(text: str) -> str:
    if not isinstance(text, str):
        return ""
    marker = "\n\n[meta]"
    if marker in text:
        return text.split(marker, 1)[0]
    return text


def generate_summary(llm, cluster_points):
    """ç”Ÿæˆæ—¥æ–‡æ‘˜è¦"""
    texts = [_strip_meta_suffix(p.payload.get('page_content', '')) for p in cluster_points]
    combined_text = "\n---\n".join(texts)[:20000] # ç•¥å¾®å¢å¤§ Context Limit
    
    template = """ã‚ãªãŸã¯ã€è“®ãƒç©ºå¥³å­¦é™¢ã‚¹ã‚¯ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¯ãƒ©ãƒ–ã€ã®ã‚·ãƒŠãƒªã‚ªç·¨é›†ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã€æ™‚ç³»åˆ—é †ã«ä¸¦ã‚“ã ä¸€é€£ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ–­ç‰‡ã§ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘:
ã“ã‚Œã‚‰ã®æ–­ç‰‡ã‚’çµ±åˆã—ã€**ä½•ãŒèµ·ããŸã‹ã€èª°ãŒã©ã†æ„Ÿã˜ãŸã‹ã€é‡è¦ãªå¤‰åŒ–ã¯ä½•ã‹**ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€åˆ¶ç´„äº‹é …ï¼ˆå³å®ˆï¼‰ã€‘:
1. **å‡ºåŠ›ã¯è¦ç´„æœ¬æ–‡ã®ã¿**ï¼ˆæŒ¨æ‹¶ä¸è¦ï¼‰ã€‚
2. æ—¥æœ¬èªã§è¨˜è¿°ã€‚
3. 200ã€œ500æ–‡å­—ç¨‹åº¦ã€‚
4. æŠ½è±¡çš„ãªè¡¨ç¾ã ã‘ã§ãªãã€å…·ä½“çš„ãªå‡ºæ¥äº‹ï¼ˆä¾‹ï¼šèª°ãŒã©ã“ã«è¡Œã£ãŸã€ä½•ã®æ›²ã‚’ç·´ç¿’ã—ãŸï¼‰ã‚’å«ã‚ã‚‹ã“ã¨ã€‚

ã€ãƒ†ã‚­ã‚¹ãƒˆæ–­ç‰‡ã€‘:
{context}

ã€è¦ç´„å‡ºåŠ›ã€‘:
"""
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    
    try:
        return chain.invoke({"context": combined_text}).strip()
    except Exception as e:
        print(f"âš ï¸ Summary Error: {e}")
        return "è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼"

def main():
    client = QdrantClient(url=QDRANT_URL)
    
    print("\n" + "="*50)
    print("ğŸ› ï¸  Build Hierarchy Index (Smart Incremental Mode)")
    print("="*50)
    
    # 1. æ‹‰å–æ‰€æœ‰æ•°æ® (åŒ…æ‹¬åˆšåˆš Append è¿›å»çš„)
    all_points = fetch_all_vectors(client, SOURCE_COLLECTION)
    if not all_points: return

    # 2. é‡æ–°è¿›è¡Œå…¨å±€èšç±»
    # æ³¨æ„ï¼šå³ä¾¿åªåŠ äº†1æ¡æ•°æ®ï¼Œä¸ºäº†ä¿è¯æ—¶é—´åˆ‡åˆ†çš„å…¨å±€æœ€ä¼˜ï¼Œæˆ‘ä»¬ä¹Ÿé‡æ–°è·‘ä¸€æ¬¡èšç±»ã€‚
    # å› ä¸ºèšç±»å¾ˆå¿«ï¼ˆå‡ ç§’é’Ÿï¼‰ï¼Œè€Œä¸”ä¸èŠ±é’±ã€‚çœŸæ­£èŠ±é’±çš„æ˜¯åé¢çš„ LLMã€‚
    clusters = perform_clustering(all_points)
    
    # 3. å‡†å¤‡æ‘˜è¦é›†åˆ
    if not client.collection_exists(SUMMARY_COLLECTION):
        print(f"ğŸ†• åˆ›å»ºæ–°æ‘˜è¦é›†åˆ...")
        client.create_collection(
            collection_name=SUMMARY_COLLECTION,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
    
    # åˆå§‹åŒ–æ¨¡å‹
    llm = ChatOpenAI(
        base_url=LLM_BASE_URL, api_key=LLM_API_KEY, model=LLM_MODEL_NAME, temperature=0.1
    )
    embed_model = XinferenceEmbeddings(server_url=XINFERENCE_URL, model_uid="bge-m3")

    print("\nğŸ“ æ™ºèƒ½åŒæ­¥æ‘˜è¦ (Smart Syncing)...")
    
    stats_skipped = 0
    stats_updated = 0
    
    total_clusters = len(clusters)

    for cluster_id, points in clusters.items():
        # A. æ„å»ºå½“å‰ç°‡çš„â€œæŒ‡çº¹â€ (ä½¿ç”¨åŒ…å«çš„å­ç‰‡æ®µ ID åˆ—è¡¨)
        # å¦‚æœå­ç‰‡æ®µåˆ—è¡¨å®Œå…¨ä¸€è‡´ï¼Œè¯´æ˜è¿™ä¸ªç°‡çš„å†…å®¹æ²¡æœ‰å˜
        current_child_ids = [p.id for p in points]
        
        # B. æ£€æŸ¥æ•°æ®åº“é‡Œæ˜¯å¦å·²æœ‰è¿™ä¸ª Cluster ID
        existing_points = client.retrieve(
            collection_name=SUMMARY_COLLECTION,
            ids=[cluster_id],
            with_payload=True
        )
        
        should_generate = True
        
        if existing_points:
            existing_payload = existing_points[0].payload
            existing_child_ids = existing_payload.get('child_ids', [])
            
            # [æ ¸å¿ƒé€»è¾‘] å¯¹æ¯”æŒ‡çº¹
            # æ³¨æ„ï¼šåˆ—è¡¨æ¯”è¾ƒè¦æ±‚é¡ºåºä¸€è‡´ï¼Œæˆ‘ä»¬å‡è®¾ points å·²ç»æŒ‰æ—¶é—´æ’å¥½åº
            if current_child_ids == existing_child_ids:
                should_generate = False
                print(f"   â© Cluster {cluster_id+1}/{total_clusters} æ— å˜åŒ– (Skipped).")
                stats_skipped += 1
            else:
                print(f"   ğŸ”„ Cluster {cluster_id+1}/{total_clusters} å†…å®¹å˜æ›´/åç§»ï¼Œéœ€æ›´æ–° (Updating)...")
                # å¯èƒ½æ˜¯å› ä¸ºæ–°æ•°æ®æ’å…¥å¯¼è‡´åˆ‡åˆ†ç‚¹å˜äº†ï¼Œæˆ–è€…è¿½åŠ äº†æ–°æ•°æ®
        else:
            print(f"   ğŸ†• Cluster {cluster_id+1}/{total_clusters} æ˜¯æ–°ç°‡ (New)...")

        # C. å¦‚æœéœ€è¦ç”Ÿæˆ (æ–°ç°‡ æˆ– å†…å®¹å˜æ›´)
        if should_generate:
            try:
                # 1. ç”Ÿæˆæ‘˜è¦
                summary_text = generate_summary(llm, points)
                
                # 2. å‘é‡åŒ–
                vector = embed_model.embed_query(summary_text)
                
                # 3. å­˜å…¥ (Upsert ä¼šè¦†ç›–æ—§ ID)
                point = PointStruct(
                    id=cluster_id, 
                    vector=vector,
                    payload={
                        "page_content": summary_text,
                        "child_ids": current_child_ids, # å­˜å…¥æ–°æŒ‡çº¹
                        "level": "summary",
                        "count": len(points)
                    }
                )
                client.upsert(collection_name=SUMMARY_COLLECTION, points=[point])
                stats_updated += 1
                
                # ç¨å¾®é¢„è§ˆä¸€ä¸‹
                print(f"      -> æ‘˜è¦ç”Ÿæˆå®Œæ¯•: {summary_text[:30]}...")
                
            except Exception as e:
                print(f"âŒ Error on cluster {cluster_id}: {e}")
                time.sleep(2)

    print(f"\nğŸ‰ åŒæ­¥å®Œæˆï¼")
    print(f"   - è·³è¿‡ (æ— å˜åŒ–): {stats_skipped}")
    print(f"   - æ›´æ–°/æ–°å¢: {stats_updated}")

if __name__ == "__main__":
    main()
