import os
import json
import time
import warnings

# å±è”½çƒ¦äººçš„è­¦å‘Š
warnings.filterwarnings("ignore")

# --- æ ¸å¿ƒç»„ä»¶ ---
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- æ£€ç´¢ç»„ä»¶ ---
from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
from langchain_community.embeddings import XinferenceEmbeddings
from qdrant_client import QdrantClient

# --- è‡ªå®šä¹‰ç»„ä»¶ ---
try:
    from reranker import XinferenceRerank
except ImportError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° reranker.py")
    exit(1)

try:
    from langchain.retrievers import ContextualCompressionRetriever
except ImportError:
    from langchain_classic.retrievers import ContextualCompressionRetriever

# ==============================================================================
# ğŸ› ï¸ é…ç½®åŒºåŸŸ (è¯·ç¡®ä¿ä¸ build_hierarchy.py ä¸€è‡´)
# ==============================================================================

# 1. æ£€ç´¢åç«¯ (Xinference)
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
EMBED_MODEL = "bge-m3"
RERANK_MODEL = "bge-reranker-v2-m3"

# 2. å‘é‡æ•°æ®åº“ (Qdrant)
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
RAW_COLLECTION_NAME = "story_knowledge_base"  # ç»†èŠ‚ç¢ç‰‡
SUMMARY_COLLECTION_NAME = "story_summary_store" # å®è§‚æ‘˜è¦
SPARSE_VECTOR_NAME = "langchain-sparse"

# 3. ç”Ÿæˆåç«¯ (Gemini Proxy)
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.example.com/v1")
LLM_API_KEY = os.getenv("LLM_API_KEY", "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "your-")

# ==============================================================================

def format_docs(docs, is_summary=False):
    """
    æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼Œå…¼å®¹ç»†èŠ‚ç¢ç‰‡ (åŒ…å«JSON metadata) å’Œå®è§‚æ‘˜è¦ã€‚
    [Fix] ä¼˜åŒ– format_docs ä»¥é€‚åº”ä¸¤ç§æ•°æ®æ ¼å¼ã€‚
    """
    formatted = []
    for i, doc in enumerate(docs):
        meta = doc.metadata.copy()
        score = meta.pop("relevance_score", 0)
        
        # 1. å¤„ç†ç»†èŠ‚ç¢ç‰‡ (åŒ…å«å¤æ‚çš„ JSON metadata)
        if not is_summary:
            # æ™ºèƒ½è§£æåµŒå¥— JSON
            for key, value in meta.items():
                if isinstance(value, str) and (value.startswith("{") or value.startswith("[")):
                    try:
                        meta[key] = json.loads(value)
                    except:
                        pass
            
            meta_json = json.dumps(meta, ensure_ascii=False, indent=2)
            content = (
                f"<fragment id='{i+1}' score='{score:.4f}'>\n"
                f"  <content>\n{doc.page_content}\n  </content>\n"
                f"  <metadata>\n{meta_json}\n  </metadata>\n"
                f"</fragment>"
            )
        
        # 2. å¤„ç†å®è§‚æ‘˜è¦ (page_content å³æ‘˜è¦æœ¬èº«)
        else:
            # æ‘˜è¦çš„ metadata è¾ƒç®€å•ï¼Œæˆ‘ä»¬åªä¿ç•™å…³é”®ä¿¡æ¯
            level = meta.get("level", "summary")
            child_count = meta.get("count", 0)
            
            content = (
                f"<summary_topic id='{i+1}' score='{score:.4f}' level='{level}' child_count='{child_count}'>\n"
                f"  <topic_summary>\n{doc.page_content}\n  </topic_summary>\n"
                f"</summary_topic>"
            )

        formatted.append(content)
        
    return "\n\n".join(formatted)

def main():
    print(f"\nå¯åŠ¨åˆ†å±‚æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (Hierarchical RAG)...")
    
    # 1. åˆå§‹åŒ–è¿æ¥
    client = QdrantClient(url=QDRANT_URL)
    dense_emb = XinferenceEmbeddings(server_url=XINFERENCE_URL, model_uid=EMBED_MODEL)
    sparse_emb = FastEmbedSparse(model_name="Qdrant/bm25")
    
    # --- åˆå§‹åŒ–ä¸¤ä¸ª Vector Store ---
    raw_store = QdrantVectorStore(
        client=client,
        collection_name=RAW_COLLECTION_NAME,
        embedding=dense_emb,
        sparse_embedding=sparse_emb,
        sparse_vector_name=SPARSE_VECTOR_NAME,
        retrieval_mode=RetrievalMode.HYBRID # ç»†èŠ‚ï¼šHybrid Search
    )
    
    # æ£€æŸ¥æ‘˜è¦é›†åˆæ˜¯å¦å­˜åœ¨
    if not client.collection_exists(SUMMARY_COLLECTION_NAME):
        print(f"âš ï¸ æ³¨æ„ï¼šå®è§‚æ‘˜è¦é›†åˆ '{SUMMARY_COLLECTION_NAME}' ä¸å­˜åœ¨ï¼")
        print("    è¯·å…ˆè¿è¡Œ 'build_hierarchy.py' æ¥ç”Ÿæˆæ‘˜è¦ç´¢å¼•ã€‚")
        summary_store = None
    else:
        summary_store = QdrantVectorStore(
            client=client,
            collection_name=SUMMARY_COLLECTION_NAME,
            embedding=dense_emb,
            retrieval_mode=RetrievalMode.DENSE # æ‘˜è¦ï¼šDense Search å³å¯
        )


    # 2. åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        base_url=LLM_BASE_URL,
        api_key=LLM_API_KEY,
        model=LLM_MODEL_NAME,
        temperature=0.7, # ç”¨äºç”Ÿæˆå›ç­”
        streaming=True,
        max_tokens=10240
    )

    rewrite_llm = ChatOpenAI(
        base_url=LLM_BASE_URL,
        api_key=LLM_API_KEY,
        model=LLM_MODEL_NAME,
        temperature=0.0, # ç”¨äºç¿»è¯‘å’Œåˆ†ç±»ï¼Œä¿è¯ç¨³å®šæ€§
        streaming=False
    )
    
    # =========================================================
    # 1. æ„å›¾åˆ†ç±»é“¾ (Intent Classification Chain)
    # =========================================================
    intent_template = """åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œåˆ¤æ–­å®ƒæ˜¯å¯¹å‰§æƒ…çš„â€œå®è§‚æ€»ç»“â€è¿˜æ˜¯â€œç»†èŠ‚è¿½é—®â€ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{query}

ã€åˆ¤æ–­è§„åˆ™ã€‘ï¼š
1. å®è§‚æ€»ç»“ (overview)ï¼šåŒ…å«â€œæ¦‚æ‹¬â€ã€â€œæ€»ç»“â€ã€â€œå¤§æ„â€ã€â€œè®²äº†ä»€ä¹ˆâ€ã€â€œèƒŒæ™¯â€ç­‰è¯ï¼Œæˆ–è¯¢é—®ä¸»é¢˜/ç¯‡ç« å†…å®¹ã€‚
2. ç»†èŠ‚è¿½é—® (specific)ï¼šè¯¢é—®å…·ä½“è§’è‰²ã€å…·ä½“å°è¯ã€å…·ä½“æ—¶é—´ã€å…·ä½“åŠ¨ä½œã€æˆ–å¯»æ‰¾è¯­éŸ³æ–‡ä»¶IDã€‚

è¯·åªè¾“å‡º 'overview' æˆ– 'specific'ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šã€‚"""
    intent_prompt = ChatPromptTemplate.from_template(intent_template)
    intent_chain = intent_prompt | rewrite_llm | StrOutputParser()

    # =========================================================
    # 2. ç»†èŠ‚æ£€ç´¢è·¯å¾„ (Specific Path)
    # =========================================================
    
    # æŸ¥è¯¢é‡å†™ Prompt (ä¸ä½ æä¾›çš„æœ€æ–°ç‰ˆä¸€è‡´)
    rewrite_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸º **RAG æ··åˆæ£€ç´¢ç³»ç»Ÿ (Hybrid Search)** è®¾è®¡çš„æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚... [æ­¤å¤„çœç•¥ï¼Œé€»è¾‘ä¸ä½ æä¾›çš„æœ€æ–°ç‰ˆæœ¬ä¸€è‡´]"""
    # ... (å°†æœ€æ–°çš„ rewrite_template æ”¾åˆ°è¿™é‡Œ)
    rewrite_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸º **RAG æ··åˆæ£€ç´¢ç³»ç»Ÿ (Hybrid Search)** è®¾è®¡çš„æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚
è¯¥ç³»ç»Ÿçš„ä¸‹æ¸¸åŒ…å«ä¸¤ä¸ªæ£€ç´¢å¼•æ“ï¼Œä½ éœ€è¦æ„é€ ä¸€ä¸ªèƒ½åŒæ—¶æ»¡è¶³å®ƒä»¬éœ€æ±‚çš„æ—¥æ–‡æŸ¥è¯¢è¯­å¥ï¼š

1. **è¯­ä¹‰æ£€ç´¢å¼•æ“ (Dense Vector / BGE-M3)**: 
   - åå¥½ï¼šå®Œæ•´çš„è‡ªç„¶è¯­è¨€å¥å­ï¼ŒåŒ…å«ä¸»è¯­ã€è°“è¯­ã€å®¾è¯­ã€‚
   - ç›®æ ‡ï¼šç†è§£â€œè°åšäº†ä»€ä¹ˆâ€ã€â€œæŸç§æ°›å›´â€ç­‰æŠ½è±¡è¯­ä¹‰ã€‚
   - è¦æ±‚ï¼š**ä¸è¦**ç ´åå¥å­çš„è¯­æ³•ç»“æ„ã€‚

2. **å…³é”®è¯æ£€ç´¢å¼•æ“ (Sparse Vector / BM25)**:
   - åå¥½ï¼šç²¾ç¡®çš„ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€æ›²åï¼‰å’Œç¨€æœ‰è¯æ±‡ã€‚
   - ç›®æ ‡ï¼šé€šè¿‡å­—é¢åŒ¹é…é€šè¿‡ IDF æœºåˆ¶è¿‡æ»¤æ— å…³æ–‡æ¡£ã€‚
   - è¦æ±‚ï¼šå¿…é¡»åŒ…å«å‡†ç¡®çš„è§’è‰²åï¼ˆå¦‚â€œä¹™å®—æ¢¢â€è€Œéâ€œæ¢¢â€ï¼‰ã€‚

3. **æœ¯è¯­ä¿®æ­£ (Term Correction)**: ç”¨æˆ·å¯èƒ½ä¼šä½¿ç”¨é”™è¯¯çš„æ±‰å­—æˆ–ä¸å‡†ç¡®çš„ç§°å‘¼ã€‚è¯·åˆ©ç”¨ä½ çš„çŸ¥è¯†åº“è¿›è¡Œä¿®æ­£ã€‚
    - ç¤ºä¾‹: "ä¸‰å‰‘å£«" -> åº”è¯¥ä¿®æ­£ä¸º "ä¸‰éŠƒå£«" (å› ä¸ºè²ä¹‹ç©ºå‰§æƒ…ä¸­å¸¸ç”¨çš„æ˜¯â€œãŠæ°—æ¥½ä¸‰éŠƒå£«â€)
    ã€è§’è‰²åæ˜ å°„è¡¨ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)ã€‘ï¼š
    - æ¢¢ / æ¢¢å‰è¾ˆ -> ä¹™å®—æ¢¢
    - èŠ±å¸† / èŠ±å¸†æ¡‘ -> æ—¥é‡ä¸‹èŠ±å¸†
    - æ…ˆ / æ…ˆå‰è¾ˆ -> è—¤å³¶æ…ˆ
    - ç‘ ç’ƒä¹ƒ -> å¤§æ²¢ç‘ ç’ƒä¹ƒ
    - åŸå­ / ç™¾ç”Ÿ / å°åŸå­ -> ç™¾ç”ŸåŸå­
    - å°é“ƒ / å¾’ç”º -> å¾’ç”ºå°éˆ´
    - å§¬èŠ½ / å®‰é¤Šå¯º -> å®‰é¤Šå¯ºå§«èŠ½
    - å¡æ‹‰æ–¯ -> ã‚»ãƒ©ã‚¹ (æˆ–æ˜µç§° ã‚»ã£ã¡ã‚ƒã‚“)

ã€æ‰§è¡Œæ­¥éª¤ã€‘ï¼š
1. **æ„å›¾ç†è§£**: ...
2. **è‡ªç„¶ç¿»è¯‘**: ...
3. **åŒä¹‰è¯æ³›åŒ– (å…³é”®)**: 
   - é’ˆå¯¹åŠ¨ä½œæˆ–æƒ…æ„Ÿæè¿°ï¼Œ**å¿…é¡»**æ‰©å±•ç›¸å…³çš„æ—¥æ–‡åŒä¹‰è¯ï¼Œä»¥ç¡®ä¿å…³é”®è¯æ£€ç´¢ (BM25) èƒ½å‘½ä¸­ä¸åŒæè¿°é£æ ¼çš„ç‰‡æ®µã€‚
   - ç¤ºä¾‹: "å“­" -> å¿…é¡»åŒ…å« `æ³£ã æ¶™ å·æ³£ æ¶™å£° æ³£ãé¡”`
   - ç¤ºä¾‹: "ç¬‘" -> å¿…é¡»åŒ…å« `ç¬‘ã† ç¬‘é¡” å¾®ç¬‘ã‚€`
4. **è¯é¢˜é”å®š**: ...

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
[ä¿®æ­£åçš„è‡ªç„¶æ—¥æ–‡é—®å¥] [åŒä¹‰è¯æ‰©å±•] [ç±»å‹å…³é”®è¯] [æ ¸å¿ƒè¯é¢˜è¯]

ã€ç¤ºä¾‹ã€‘ï¼š
ç”¨æˆ·: æ¢¢å“­äº†å‡ æ¬¡ï¼Ÿ
è¾“å‡º: ä¹™å®—æ¢¢ã¯ä½•å›æ³£ãã¾ã—ãŸã‹ï¼Ÿ æ³£ã æ¶™ å·æ³£ æ¶™å£° æ³£ãé¡” ã‚·ãƒ¼ãƒ³ (åŒ…å«æ‰€æœ‰"å“­"ç›¸å…³çš„è¯)

ç”¨æˆ·: è°åœ¨ç»ƒä¹ å®¤ç”Ÿæ°”äº†ï¼Ÿ
è¾“å‡º: ç·´ç¿’å®¤ã§èª°ãŒæ€’ã‚Šã¾ã—ãŸã‹ï¼Ÿ æ€’ã‚‹ æ¿€æ€’ ä¸æ©Ÿå«Œ å–§å˜© å ´æ‰€

ç”¨æˆ·é—®é¢˜: {question}
"""
    rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)
    # é‡å†™é“¾ä½¿ç”¨æ¸©åº¦ä¸º 0 çš„ LLM
    rewrite_chain = rewrite_prompt | rewrite_llm | StrOutputParser()
    
    # Reranker é…ç½®
    reranker = XinferenceRerank(
        url=f"{XINFERENCE_URL.rstrip('/')}/v1/rerank",
        model_uid=RERANK_MODEL,
        top_n=25,
        request_timeout=240
    )
    
    # ç»†èŠ‚æ£€ç´¢å™¨
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=reranker,
        base_retriever=raw_store.as_retriever(search_kwargs={"k": 150})
    )

    # ç»†èŠ‚ç”Ÿæˆ Prompt
    specific_answer_template = """ä½ æ˜¯ä¸€ä¸ªç²¾é€šã€Šè²ä¹‹ç©ºå¥³å­¦é™¢ã€‹å‰§æƒ…çš„ AI åŠ©æ‰‹ã€‚
è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹æ£€ç´¢åˆ°çš„ã€æ—¥æ–‡å‰§æƒ…ç‰‡æ®µã€‘ï¼Œå¹¶ç”¨**ä¸­æ–‡**å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å‰§æƒ…ç‰‡æ®µã€‘ï¼š
{context}

ã€ç”¨æˆ·åŸé—®é¢˜ã€‘ï¼š
{original_question}

ã€å›ç­”è¦æ±‚ã€‘ï¼š
1. åŸºäºæ—¥æ–‡ç‰‡æ®µçš„å†…å®¹è¿›è¡Œæ¨ç†ï¼Œä½†**å¿…é¡»ç”¨ä¸­æ–‡å›ç­”**ã€‚
2. æ²‰æµ¸å¼å›ç­”ï¼šè¯·ç›´æ¥ä»¥â€œå‰§æƒ…ä¸“å®¶â€çš„èº«ä»½å›ç­”ï¼Œ**ä¸¥ç¦**åœ¨å›ç­”ä¸­æåŠâ€œç‰‡æ®µIDâ€ã€â€œXMLç»“æ„â€æˆ–â€œæ ¹æ®æä¾›çš„JSONæ•°æ®â€ã€‚
3. å‡†ç¡®å¼•ç”¨ï¼šå¦‚æœéœ€è¦æŒ‡å‡ºå…·ä½“å‡ºå¤„ï¼Œè¯·ä½¿ç”¨å…ƒæ•°æ®ä¸­çš„ `scene` (åœºæ™¯ID) æˆ– æ—¶é—´/åœ°ç‚¹ã€‚
4. å¦‚æœæ£€ç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·ç›´æ¥è¯´â€œåœ¨å½“å‰æ£€ç´¢åˆ°çš„å‰§æƒ…ä¸­æœªæ‰¾åˆ°â€ï¼Œä¸è¦ç¼–é€ ã€‚
"""
    specific_answer_prompt = ChatPromptTemplate.from_template(specific_answer_template)

    # =========================================================
    # 3. å®è§‚æ‘˜è¦è·¯å¾„ (Overview Path)
    # =========================================================
    summary_answer_template = """ä½ æ˜¯ä¸€ä¸ªé«˜é˜¶å‰§æƒ…åˆ†æå¸ˆã€‚
ç”¨æˆ·æ­£åœ¨å¯»æ±‚å¯¹å‰§æƒ…çš„å®è§‚æ¦‚æ‹¬ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ã€ä¸»é¢˜æ‘˜è¦ç‰‡æ®µã€‘ï¼Œç”¨æµç•…çš„ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€ä¸»é¢˜æ‘˜è¦ç‰‡æ®µã€‘ï¼š
{context}

ã€ç”¨æˆ·åŸé—®é¢˜ã€‘ï¼š
{original_question}

ã€å›ç­”è¦æ±‚ã€‘ï¼š
1. ä¾§é‡äºå®è§‚å™äº‹ã€ä¸»è¦è§’è‰²å…³ç³»å’Œæ ¸å¿ƒäº‹ä»¶ï¼Œä¸è¦é™·å…¥ç»†èŠ‚ã€‚
2. ä¸¥ç¦æåŠâ€œç‰‡æ®µâ€ã€â€œæ‘˜è¦â€ã€â€œIDâ€ç­‰ç³»ç»Ÿæœ¯è¯­ã€‚
"""
    summary_answer_prompt = ChatPromptTemplate.from_template(summary_answer_template)


    # --- äº¤äº’å¾ªç¯ ---
    while True:
        print("\n" + "="*50)
        user_query = input("è¯·æé—® [qé€€å‡º]: ")
        if user_query.lower() in ['q', 'exit']: break
        
        try:
            # 1. æ„å›¾åˆ†ç±»
            print(f"æ­£åœ¨åˆ†ææ„å›¾...")
            intent = intent_chain.invoke({"query": user_query}).strip().lower()
            print(f"   åˆ†ç±»ç»“æœ: {intent}")
            
            # --- å®è§‚æ¦‚æ‹¬è·¯å¾„ ---
            if intent == 'overview' and summary_store:
                
                print("è·¯ç”±: å®è§‚æ‘˜è¦æ£€ç´¢...")
                # åœ¨æ‘˜è¦é›†åˆä¸­è¿›è¡Œæœç´¢
                docs = summary_store.similarity_search(user_query, k=3)
                context_str = format_docs(docs, is_summary=True)
                
                print("æ­£åœ¨ç”Ÿæˆå®è§‚å›ç­”...")
                print("-" * 30)
                final_chain = summary_answer_prompt | llm | StrOutputParser()

            # --- ç»†èŠ‚è¿½é—®è·¯å¾„ (é»˜è®¤è·¯å¾„) ---
            else:
                if intent == 'overview' and not summary_store:
                    print("å®è§‚æ‘˜è¦æœªå°±ç»ªï¼Œå¼ºåˆ¶è½¬ä¸ºç»†èŠ‚æ£€ç´¢ã€‚")
                
                print(f"æ­£åœ¨æå–å…³é”®è¯...")
                # 1. æ‰§è¡Œé‡å†™
                jp_query = rewrite_chain.invoke({"question": user_query})
                print(f"\rç”Ÿæˆæœç´¢è¯: {jp_query}")
                
                # 2. æ‰§è¡Œæ£€ç´¢ (ç”¨æ—¥æ–‡æœ)
                print(f"æ­£åœ¨æ£€ç´¢ (Hybrid + Rerank)...")
                docs = compression_retriever.invoke(jp_query)
                
                if not docs:
                    print("æœªæ‰¾åˆ°ç›¸å…³å‰§æƒ…ã€‚")
                    continue
                    
                # 3. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
                context_str = format_docs(docs, is_summary=False)
                
                # 4. ç”Ÿæˆå›ç­” (ç”¨ä¸­æ–‡å›)
                print(f"æ­£åœ¨ç”Ÿæˆå›ç­”...")
                print("-" * 30)
                final_chain = specific_answer_prompt | llm | StrOutputParser()

            # ç»Ÿä¸€è¾“å‡º
            for chunk in final_chain.stream({
                "context": context_str,
                "original_question": user_query
            }):
                print(chunk, end="", flush=True)
            print("\n")
            
        except Exception as e:
            print(f"\nâŒ æµç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()