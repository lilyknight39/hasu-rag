import os
import json
import warnings
import re
import sys
import asyncio
import threading
from pathlib import Path
from typing import Optional, Tuple

# å±è”½çƒ¦äººçš„è­¦å‘Š
warnings.filterwarnings("ignore")

# --- æ ¸å¿ƒç»„ä»¶ ---
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- æ£€ç´¢ç»„ä»¶ ---
from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
from langchain_community.embeddings import XinferenceEmbeddings
from qdrant_client import QdrantClient

# --- è‡ªå®šä¹‰ç»„ä»¶ ---
try:
    from reranker import XinferenceRerank
except ImportError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° reranker.py")
    exit(1)

try:
    from langchain.retrievers import ContextualCompressionRetriever
except ImportError:
    from langchain_classic.retrievers import ContextualCompressionRetriever

# ==============================================================================
# ğŸ› ï¸ é…ç½®åŒºåŸŸ (è¯·ç¡®ä¿ä¸ build_hierarchy.py ä¸€è‡´)
# ==============================================================================

# 1. æ£€ç´¢åç«¯ (Xinference)
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
EMBED_MODEL = "bge-m3"
RERANK_MODEL = "bge-reranker-v2-m3"

# 2. å‘é‡æ•°æ®åº“ (Qdrant)
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
RAW_COLLECTION_NAME = "story_knowledge_base"  # ç»†èŠ‚ç¢ç‰‡
SUMMARY_COLLECTION_NAME = "story_summary_store" # å®è§‚æ‘˜è¦
SPARSE_VECTOR_NAME = "langchain-sparse"

# 3. ç”Ÿæˆåç«¯ (LLM)
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "")
LLM_API_KEY = os.getenv("LLM_API_KEY", "")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "")

# ==============================================================================
# ğŸ”§ OneBot MCP (optional)
# ==============================================================================

ONEBOT_MCP_ENABLED = os.getenv("ONEBOT_MCP_ENABLED", "0") == "1"
ONEBOT_DEFAULT_TARGET = os.getenv("ONEBOT_DEFAULT_TARGET", "").strip()
ONEBOT_DEFAULT_TARGET_TYPE = os.getenv("ONEBOT_DEFAULT_TARGET_TYPE", "").strip().lower()
ONEBOT_DEFAULT_TARGET_ID = os.getenv("ONEBOT_DEFAULT_TARGET_ID", "").strip()
ONEBOT_MCP_SERVER_PATH = os.getenv("ONEBOT_MCP_SERVER_PATH", "").strip()
ONEBOT_SEND_MODE = os.getenv("ONEBOT_SEND_MODE", "auto").strip().lower()
ONEBOT_MULTI_SEND = os.getenv("ONEBOT_MULTI_SEND", "0") == "1"
ONEBOT_SEND_INTERVAL_SECONDS = float(os.getenv("ONEBOT_SEND_INTERVAL_SECONDS", "1.0"))

_ONEBOT_TRIGGER_RE = re.compile(r"(å‘é€|æ’­æ”¾|å‘åˆ°|å‘é€åˆ°|å‘ç»™|å‘é€ç»™)")
_ONEBOT_FILE_MODE_RE = re.compile(r"(æ–‡ä»¶|file|ä¸Šä¼ )", re.IGNORECASE)
_ONEBOT_VOICE_NAME_RE = re.compile(
    r"(?<![A-Za-z0-9_])(vo_adv_[A-Za-z0-9_@-]+(?:\.mp3)?)(?![A-Za-z0-9_])",
    re.IGNORECASE
)
_ONEBOT_FILE_EXT_RE = re.compile(
    r"(?<![A-Za-z0-9_])(vo_adv_[A-Za-z0-9_@-]+\.mp3)(?![A-Za-z0-9_])",
    re.IGNORECASE
)
_ONEBOT_TARGET_RE = re.compile(r"(?:å‘åˆ°|å‘é€åˆ°|å‘ç»™|å‘é€ç»™)\s*(ç¾¤|ç¾¤èŠ|ç¾¤é‡Œ|ç§èŠ|ç§ä¿¡|å¥½å‹)?\s*([0-9]{5,})")

# ==============================================================================
# ğŸ“ Prompt Templates (ä¾› API å’Œäº¤äº’æ¨¡å¼å…±ç”¨)
# ==============================================================================

INTENT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ„å›¾åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„å…³äºã€Šè²ä¹‹ç©ºå¥³å­¦é™¢ã€‹å‰§æƒ…çš„æé—®ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{query}

ã€åˆ†ç±»å®šä¹‰ã€‘ï¼š
1. **analysis (æ·±åº¦åˆ†æ)**: 
   - è¯¢é—®è§’è‰²ä¹‹é—´çš„**å…³ç³»ã€æ„Ÿæƒ…ã€æ€åº¦**ï¼ˆå¦‚"åŸå­æ€ä¹ˆçœ‹èŠ±å¸†"ã€"ä¸¤äººçš„å…³ç³»å˜åŒ–"ï¼‰ã€‚
   - è¯¢é—®**åŸå› ã€åŠ¨æœºã€èƒŒæ™¯**ï¼ˆå¦‚"ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåš"ã€"èƒŒåçš„å«ä¹‰"ï¼‰ã€‚
   - è¯¢é—®**æ€§æ ¼ã€è¯„ä»·ã€æˆé•¿**ã€‚
   - **ç»å¤§å¤šæ•°éçº¯äº‹å®æ£€ç´¢çš„é—®é¢˜éƒ½åº”å½’ä¸ºæ­¤ç±»ã€‚**

2. **overview (å®è§‚æ¦‚æ‹¬)**: 
   - ä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚"æ€»ç»“å…¨æ–‡"ã€"æ¦‚æ‹¬æŸç« å¤§æ„"ã€"è®²äº†ä»€ä¹ˆæ•…äº‹"æ—¶ã€‚

3. **fact (äº‹å®è¿½é—®)**: 
   - è¯¢é—®æå…¶å…·ä½“çš„**æ—¶é—´ã€åœ°ç‚¹ã€æ¬¡æ•°ã€ç‰©å“**ï¼ˆå¦‚"ç¬¬å‡ è¯å“­äº†"ã€"åƒçš„ä»€ä¹ˆ"ã€"IDæ˜¯å¤šå°‘"ï¼‰ã€‚
   - å¯»æ‰¾å…·ä½“çš„æŸå¥å°è¯å‡ºå¤„ã€‚

è¯·åªè¾“å‡ºå…¶ä¸­ä¸€ä¸ªæ ‡ç­¾ï¼š'analysis', 'overview', æˆ– 'fact'ã€‚
è‹¥ç”¨æˆ·åœ¨é—®é¢˜ä¸­æ˜ç¡®æŒ‡å®š analysisã€overviewã€factï¼Œä¼˜å…ˆä½¿ç”¨è¯¥æ ‡ç­¾ã€‚
"""

DENSE_REWRITE_TEMPLATE = """ä½ æ˜¯ä¸º **BGE-M3 è¯­ä¹‰æ£€ç´¢** æœåŠ¡çš„æŸ¥è¯¢ä¼˜åŒ–å™¨ã€‚
å°†ç”¨æˆ·é—®é¢˜æ”¹å†™ä¸ºä¸€æ¡è‡ªç„¶ã€å®Œæ•´çš„æ—¥æ–‡é—®å¥ï¼Œä¿æŒä¸»è°“å®¾å’Œè¯­å¢ƒï¼Œä¸è¦æ‹†æˆå…³é”®è¯ã€‚
çº æ­£å¸¸è§è§’è‰²åï¼Œä½¿ç”¨ä¸‹æ–¹æ˜ å°„è¡¨çš„å…¨åï¼›å¦‚ç”¨æˆ·å·²æœ‰æ­£ç¡®æ—¥æ–‡åï¼Œä¿æŒä¸å˜ã€‚
è‹¥ç”¨æˆ·åªç»™å‡ºçŸ­è¯­/å…³é”®è¯ï¼Œè¯·è¡¥å…¨æˆé€šé¡ºçš„é—®å¥ï¼Œä½†ä¸è¦æ·»åŠ æ— å…³ä¿¡æ¯ã€‚
ã€è§’è‰²åæ˜ å°„è¡¨ã€‘æ¢¢/æ¢¢å‰è¾ˆ->ä¹™å®—æ¢¢ï¼›èŠ±å¸†/èŠ±å¸†æ¡‘->æ—¥é‡ä¸‹èŠ±å¸†ï¼›æ…ˆ/æ…ˆå‰è¾ˆ->è—¤å³¶æ…ˆï¼›ç‘ ç’ƒä¹ƒ->å¤§æ²¢ç‘ ç’ƒä¹ƒï¼›åŸå­/ç™¾ç”Ÿ/å°åŸå­->ç™¾ç”ŸåŸå­ï¼›å°é“ƒ/å¾’ç”º->å¾’ç”ºå°éˆ´ï¼›å§¬èŠ½/å®‰é¤Šå¯º->å®‰é¤Šå¯ºå§«èŠ½ï¼›å¡æ‹‰æ–¯->ã‚»ãƒ©ã‚¹ã€‚
ã€è¾“å‡ºç¤ºä¾‹ã€‘è¾“å…¥: æ¢¢å“­äº†å‡ æ¬¡ï¼Ÿ è¾“å‡º: ä¹™å®—æ¢¢ã¯ä½•å›æ³£ãã¾ã—ãŸã‹ï¼Ÿ
åªè¾“å‡ºæ”¹å†™åçš„æ—¥æ–‡é—®å¥ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦è¿½åŠ å…¶ä»–å­—æ®µã€‚

ç”¨æˆ·é—®é¢˜: {question}
"""

SPARSE_KEYWORD_TEMPLATE = """ä½ æ˜¯ä¸º **BM25 å…³é”®è¯æ£€ç´¢** ç”ŸæˆæŸ¥è¯¢çš„ä¸“å®¶ã€‚
è¾“å‡ºä¸€ä¸²æ—¥æ–‡/å‡åå…³é”®è¯ï¼Œåé‡äººåã€åœ°åã€é“å…·åã€æ›²åã€ç¨€æœ‰è¯ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†éš”ã€‚
åŸåˆ™ï¼š
- è§’è‰²åç”¨å…¨åï¼›ç›®æ ‡åè¯ç”¨å…·ä½“è¡¨è®°ï¼ˆå«é‡è¦è¯å½¢å˜ä½“ï¼Œå¦‚ åŠ¨è¯/è¡¨æƒ…/åˆ«ç§°ï¼‰ã€‚
- è®¡æ•°ç±»è¯æœ€å¤šä¿ç•™ 1 ä¸ªï¼ˆå¦‚ã€Œä½•å›ã€æˆ–ã€Œå›æ•°ã€ï¼‰ï¼Œä¸è¦å †å ã€‚
- é¿å…æ³›æ³›çš„ã€Œã‚·ãƒ¼ãƒ³/ç™»å ´/å ´é¢ã€ç­‰ä½ IDF è¯ï¼Œé™¤éç”¨æˆ·è¦æ±‚ã€‚
- è‹¥ç”¨æˆ·æ˜ç¡®ç¦æ­¢æŸè¯è¡¨è®°ï¼Œéµä»ç”¨æˆ·æŒ‡ç¤ºã€‚
è¦æ±‚ï¼š
- çº æ­£å¸¸è§è§’è‰²åï¼Œä½¿ç”¨ä¸‹æ–¹æ˜ å°„è¡¨çš„å…¨åã€‚
- å¯ä»¥åŠ å…¥åŒä¹‰è¯/å½¢æ€å˜åŒ–ï¼Œä½†ä¿æŒå…³é”®è¯å½¢å¼ï¼Œé¿å…å®Œæ•´å¥å­ã€‚
- è‹¥ç”¨æˆ·é—®é¢˜åŒ…å«ä¸­æ–‡æˆ–ç¼–å·ï¼Œè¯·ä¿ç•™ã€‚
ã€è§’è‰²åæ˜ å°„è¡¨ã€‘æ¢¢/æ¢¢å‰è¾ˆ->ä¹™å®—æ¢¢ï¼›èŠ±å¸†/èŠ±å¸†æ¡‘->æ—¥é‡ä¸‹èŠ±å¸†ï¼›æ…ˆ/æ…ˆå‰è¾ˆ->è—¤å³¶æ…ˆï¼›ç‘ ç’ƒä¹ƒ->å¤§æ²¢ç‘ ç’ƒä¹ƒï¼›åŸå­/ç™¾ç”Ÿ/å°åŸå­->ç™¾ç”ŸåŸå­ï¼›å°é“ƒ/å¾’ç”º->å¾’ç”ºå°éˆ´ï¼›å§¬èŠ½/å®‰é¤Šå¯º->å®‰é¤Šå¯ºå§«èŠ½ï¼›å¡æ‹‰æ–¯->ã‚»ãƒ©ã‚¹ã€‚
ã€è¾“å‡ºç¤ºä¾‹ã€‘è¾“å…¥: æ¢¢å“­äº†å‡ æ¬¡ï¼Ÿ è¾“å‡º: ä¹™å®—æ¢¢ æ³£ã æ¶™ å›æ•°
æ ¼å¼ï¼šä»…è¾“å‡ºå…³é”®è¯ä¸²ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†éš”ï¼Œä¸è¦æ·»åŠ è§£é‡Šæˆ–å…¶ä»–å†…å®¹ã€‚

ç”¨æˆ·é—®é¢˜: {question}
"""

ALPHA_TEMPLATE = """ä½ æ˜¯æ··åˆæ£€ç´¢å‚æ•°é¡¾é—®ï¼Œéœ€è¦ä¸º Dense+BM25 æ··åˆæ£€ç´¢è¾“å‡ºä¸€ä¸ª alpha å€¼ (0.15~0.65)ï¼š
- alpha å° -> æ›´ä¾èµ– BM25ï¼›alpha å¤§ -> æ›´ä¾èµ–è¯­ä¹‰ã€‚
- è‹¥é—®é¢˜å«æ˜ç¡®ç¼–å·/ID/æ•°å­—æˆ–çŸ­å…³é”®è¯ï¼Œå BM25 (0.2~0.35)ï¼›
- è‹¥é—®é¢˜æ˜¯é•¿å¥ã€å…³ç³»/å› æœåˆ†æï¼Œåè¯­ä¹‰ (0.4~0.55)ã€‚
ã€è¾“å‡ºç¤ºä¾‹ã€‘ç¤ºä¾‹1: 0.3  ç¤ºä¾‹2: 0.45
è¯·ç»“åˆåŸå§‹ä¸­æ–‡å’Œä¸¤ç§é‡å†™ï¼Œè¾“å‡ºä¸€ä¸ªæ•°å­—ï¼ˆä¾‹å¦‚ 0.3 æˆ– 0.45ï¼‰ï¼Œç¦æ­¢è¾“å‡ºå…¶ä»–å­—ç¬¦ã€‚

ã€åŸå§‹é—®é¢˜ã€‘ï¼š{original}
ã€è¯­ä¹‰é‡å†™ã€‘ï¼š{dense}
ã€å…³é”®è¯é‡å†™ã€‘ï¼š{sparse}
"""

ANSWER_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªç²¾é€šã€Šè²ä¹‹ç©ºå¥³å­¦é™¢ã€‹å‰§æƒ…çš„ä¸“å®¶çº§ AI å‰§æƒ…åˆ†æå¸ˆã€‚
ä¸ºäº†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºä½ æä¾›äº†æ£€ç´¢åˆ°çš„å‰§æƒ…ä¿¡æ¯ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«ï¼š
1. **<summary_section>**: å‰§æƒ…çš„å®è§‚æ‘˜è¦ï¼ˆæ¦‚æ‹¬èƒŒæ™¯ã€å¤§çº²ï¼‰ã€‚
2. **<story_fragment>**: å…·ä½“çš„å¯¹è¯ã€ç‹¬ç™½å’Œåœºæ™¯ç»†èŠ‚ï¼ˆç²¾ç¡®è¯æ®ï¼‰ã€‚

ã€å‰§æƒ…ç‰‡æ®µã€‘ï¼š
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{original_question}

ã€å›ç­”è¦æ±‚ã€‘ï¼š
1. **æ·±åº¦èåˆä¸è¯æ®å±•ç¤º (Evidence-Based)**ï¼š
   - è¯·ç»¼åˆ **å®è§‚èƒŒæ™¯** (Summary) ä¸ **å¾®è§‚ç»†èŠ‚** (Fragment) è¿›è¡Œä½œç­”ã€‚
   - æ—¢ç„¶ç”¨æˆ·çœ‹ä¸åˆ°åŸæ–‡ï¼Œä½ éœ€**å¤è¿°**å…³é”®å°è¯ã€åŠ¨ä½œæå†™æˆ–å¿ƒç†æ´»åŠ¨ä½œä¸ºè®ºæ®ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»™å‡ºä¸€ä¸ªç»“è®ºã€‚
   - ä¼˜å…ˆå¼•ç”¨å…·ä½“çš„å¯¹è¯å’ŒåŠ¨ä½œç»†èŠ‚ï¼Œæ‘˜è¦ä»…ä½œä¸ºèƒŒæ™¯è¡¥å……ã€‚

2. **è‡ªç„¶çš„éšå½¢å¼•ç”¨ (Natural Citation)**ï¼š
   - **ç»å¯¹ç¦æ­¢**ä½¿ç”¨ "F-X", "S-X", "ç‰‡æ®µ1", "ID:xxx" è¿™ç§æœºæ¢°ç´¢å¼•ã€‚
   - âœ… **æ­£ç¡®ç¤ºèŒƒ**ï¼š
     - "åœ¨ç»ƒä¹ å®¤çš„å†²çªåœºæ™¯ä¸­ (story_main_10500701_scene_005)ï¼ŒèŠ±å¸†å“­ç€è¯´..."
     - "æ­£å¦‚ä¹‹å‰æåˆ°çš„å§¬èŠ½å¤±å»æŒšå‹çš„ç»å†..."
     - "å½“ä¸¤äººåœ¨é’“é±¼åœºç‹¬å¤„æ—¶ï¼Œå§¬èŠ½æåˆ°..."
   - âŒ **é”™è¯¯ç¤ºèŒƒ**ï¼š
     - "æ ¹æ® F-1ï¼Œå¥¹ä»¬åµæ¶äº†ã€‚"
     - "å‚è€ƒæ‘˜è¦ section 2..."
   - *æ³¨ï¼šè‹¥æåŠç‰‡æ®µï¼Œ**å¯ä»¥**ä¿ç•™å…·ä½“çš„ Scene ID (å¦‚ story_main_... ) ä»¥ä¾¿æº¯æºï¼Œä½†è¦åµŒå…¥åœ¨è‡ªç„¶è¯­å¥ä¸­ã€‚*

3. **ç»“æ„åŒ–å›ç­”**ï¼š
   - é€»è¾‘æ¸…æ™°ï¼Œè§‚ç‚¹æ˜ç¡®ã€‚
   - æ¯ä¸€æ¡è®ºç‚¹éƒ½åº”å½“æœ‰å…·ä½“çš„å‰§æƒ…ç»†èŠ‚ï¼ˆå°è¯/åŠ¨ä½œï¼‰æ”¯æ’‘ã€‚

4. **å…œåº•ç­–ç•¥**ï¼š
   - å¦‚æœæ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸­æ²¡æœ‰ä»»ä½•ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹ï¼Œè¯·ç›´æ¥å›ç­”ï¼š"åœ¨å½“å‰æ£€ç´¢åˆ°çš„å‰§æƒ…ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"ï¼Œä¸è¦ç¼–é€ ã€‚

5. **è¯­è¨€è¦æ±‚**ï¼šå¿…é¡»ç”¨**ä¸­æ–‡**å›ç­”ã€‚
"""

# ==============================================================================

def _get_mcp_server_path() -> Path:
    if ONEBOT_MCP_SERVER_PATH:
        return Path(ONEBOT_MCP_SERVER_PATH).expanduser().resolve()
    return (Path(__file__).parent / "mcp_onebot_server.py").resolve()


def _parse_target_spec(raw: str) -> Optional[Tuple[str, int]]:
    raw = raw.strip()
    if not raw:
        return None
    if ":" in raw:
        prefix, id_str = raw.split(":", 1)
        prefix = prefix.strip().lower()
        id_str = id_str.strip()
    else:
        prefix = ""
        id_str = raw

    try:
        target_id = int(id_str)
    except ValueError:
        return None

    if prefix in ("group", "g", "grp"):
        target_type = "group"
    elif prefix in ("private", "p", "user", "u"):
        target_type = "private"
    elif prefix in ("onebot", "qq"):
        target_type = "group"
    elif prefix == "":
        target_type = "group"
    else:
        return None

    return target_type, target_id


def _get_default_target() -> Optional[Tuple[str, int]]:
    if ONEBOT_DEFAULT_TARGET:
        return _parse_target_spec(ONEBOT_DEFAULT_TARGET)
    if ONEBOT_DEFAULT_TARGET_TYPE and ONEBOT_DEFAULT_TARGET_ID:
        try:
            target_id = int(ONEBOT_DEFAULT_TARGET_ID)
        except ValueError:
            return None
        if ONEBOT_DEFAULT_TARGET_TYPE in ("group", "private"):
            return ONEBOT_DEFAULT_TARGET_TYPE, target_id
    return None


def _extract_target_from_query(query: str) -> Optional[Tuple[str, int]]:
    match = _ONEBOT_TARGET_RE.search(query)
    if not match:
        return None
    hint = (match.group(1) or "").strip()
    target_id = int(match.group(2))
    if hint in ("ç§èŠ", "ç§ä¿¡", "å¥½å‹"):
        return "private", target_id
    return "group", target_id


def _should_trigger_onebot(query: str) -> bool:
    return bool(_ONEBOT_TRIGGER_RE.search(query))


def _select_send_mode(query: str) -> str:
    if ONEBOT_SEND_MODE in ("voice", "file"):
        return ONEBOT_SEND_MODE
    return "file" if _ONEBOT_FILE_MODE_RE.search(query) else "voice"


def _dedupe_preserve(items):
    seen = set()
    result = []
    for item in items:
        if item in seen:
            continue
        seen.add(item)
        result.append(item)
    return result


def _extract_all_voice_names_from_text(text: str) -> list[str]:
    if not text:
        return []
    matches = [m.group(1) for m in _ONEBOT_VOICE_NAME_RE.finditer(text)]
    if not matches:
        matches = [m.group(1) for m in _ONEBOT_FILE_EXT_RE.finditer(text)]
    return _dedupe_preserve(matches)


def _extract_voice_name_from_text(text: str) -> Optional[str]:
    names = _extract_all_voice_names_from_text(text)
    return names[0] if names else None


def _extract_voice_names_from_docs(docs) -> list[str]:
    results = []
    for doc in docs or []:
        meta = getattr(doc, "metadata", {}) or {}
        voices = meta.get("voices") or meta.get("voice")
        ctx = meta.get("ctx")

        if not voices and ctx:
            if isinstance(ctx, str) and (ctx.startswith("{") or ctx.startswith("[")):
                try:
                    ctx = json.loads(ctx)
                except Exception:
                    ctx = None
            if isinstance(ctx, dict):
                voices = ctx.get("voices") or ctx.get("voice")

        if isinstance(voices, str):
            parsed = _extract_all_voice_names_from_text(voices)
            if parsed:
                results.extend(parsed)
                continue
            try:
                voices = json.loads(voices)
            except Exception:
                voices = [voices]

        if isinstance(voices, list):
            for voice in voices:
                if isinstance(voice, str):
                    parsed = _extract_all_voice_names_from_text(voice)
                    if parsed:
                        results.extend(parsed)

        content = getattr(doc, "page_content", "")
        parsed = _extract_all_voice_names_from_text(content)
        if parsed:
            results.extend(parsed)

    return _dedupe_preserve(results)


def _onebot_log(message: str) -> None:
    print(f"[OneBot] {message}")


def _snippet(text: Optional[str], limit: int = 120) -> str:
    if not text:
        return ""
    cleaned = " ".join(text.strip().split())
    if len(cleaned) > limit:
        return f"{cleaned[:limit - 3]}..."
    return cleaned


async def _call_onebot_mcp_tool(
    file_names: list[str],
    target_type: str,
    target_id: int,
    mode: str,
    interval_seconds: float
) -> None:
    try:
        from mcp import ClientSession, StdioServerParameters
        from mcp.client.stdio import stdio_client
    except Exception:
        try:
            from mcp.client import ClientSession, StdioServerParameters
            from mcp.client.stdio import stdio_client
        except Exception as exc:
            _onebot_log(f"MCP client not available: {exc}")
            return

    server_path = _get_mcp_server_path()
    if not server_path.exists():
        _onebot_log(f"MCP server script not found: {server_path}")
        return

    server_params = StdioServerParameters(
        command=sys.executable,
        args=[str(server_path)],
        env=os.environ.copy()
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            tool_name = "send_file" if mode == "file" else "send_voice"
            total = len(file_names)
            for idx, file_name in enumerate(file_names, start=1):
                payload = {
                    "file_name": file_name,
                    "target_type": target_type,
                    "target_id": target_id
                }
                _onebot_log(
                    f"calling MCP tool={tool_name} target={target_type}:{target_id} "
                    f"file={file_name} ({idx}/{total})"
                )
                await session.call_tool(tool_name, payload)
                if idx < total and interval_seconds > 0:
                    await asyncio.sleep(interval_seconds)
            _onebot_log("MCP tool call finished")


def _call_onebot_mcp_tool_safe(
    file_names: list[str],
    target_type: str,
    target_id: int,
    mode: str,
    interval_seconds: float
) -> None:
    try:
        asyncio.run(_call_onebot_mcp_tool(file_names, target_type, target_id, mode, interval_seconds))
    except Exception as exc:
        _onebot_log(f"MCP call failed: {exc}")


def _maybe_trigger_onebot_tool(user_query: str, answer_text: str, docs=None) -> None:
    trigger_match = _ONEBOT_TRIGGER_RE.search(user_query)
    if not trigger_match:
        return

    if not ONEBOT_MCP_ENABLED:
        _onebot_log(f"trigger found '{trigger_match.group(0)}' but MCP disabled; skip")
        return

    _onebot_log(f"trigger matched: '{trigger_match.group(0)}'")

    target_from_query = _extract_target_from_query(user_query)
    target = target_from_query or _get_default_target()
    if not target:
        _onebot_log("no target configured; set ONEBOT_DEFAULT_TARGET or ONEBOT_DEFAULT_TARGET_TYPE/ID")
        return

    target_source = "query" if target_from_query else "default"
    target_type, target_id = target
    _onebot_log(f"target: {target_type}:{target_id} (source={target_source})")

    file_names = _extract_all_voice_names_from_text(answer_text)
    file_source = "answer"
    if not file_names:
        file_names = _extract_all_voice_names_from_text(user_query)
        file_source = "query"
    if not file_names:
        file_names = _extract_voice_names_from_docs(docs)
        file_source = "docs"
    if not file_names:
        _onebot_log(
            "voice file name not found; "
            f"answer_snippet='{_snippet(answer_text)}'; "
            f"query_snippet='{_snippet(user_query)}'; "
            f"docs={len(docs) if docs else 0}"
        )
        return

    mode = _select_send_mode(user_query)
    if ONEBOT_SEND_MODE in ("voice", "file"):
        _onebot_log(f"send mode forced by env: {mode}")
    if not ONEBOT_MULTI_SEND and len(file_names) > 1:
        _onebot_log(f"multi send disabled; {len(file_names)} matches, using first only")
        file_names = [file_names[0]]
    else:
        _onebot_log(f"multi send enabled; sending {len(file_names)} file(s)")
    _onebot_log(f"voice file(s): {', '.join(file_names)} (source={file_source})")
    _onebot_log(f"send mode: {mode}")
    _onebot_log(f"dispatching MCP tool call (interval={ONEBOT_SEND_INTERVAL_SECONDS}s)")
    thread = threading.Thread(
        target=_call_onebot_mcp_tool_safe,
        args=(file_names, target_type, target_id, mode, ONEBOT_SEND_INTERVAL_SECONDS),
        daemon=True
    )
    thread.start()


def format_docs(docs):
    """
    æ™ºèƒ½æ ¼å¼åŒ–ï¼šæ··åˆå¤„ç† 'æ‘˜è¦(Summary)' å’Œ 'åŸå§‹ç‰‡æ®µ(Fragment)'
    [ä¼˜åŒ–]: ç§»é™¤å¯¹ LLM è¯±å¯¼æ€§å¼ºçš„ F-x IDï¼Œæ”¹ç”¨è¯­ä¹‰åŒ–æ ‡ç­¾
    """
    formatted = []
    for i, doc in enumerate(docs):
        meta = doc.metadata.copy()
        score = meta.pop("relevance_score", 0)
        
        # åˆ¤æ–­æ˜¯æ‘˜è¦è¿˜æ˜¯åŸå§‹ç‰‡æ®µ
        is_summary = meta.get("level") == "summary"
        
        if is_summary:
            # --- æ ¼å¼ A: æ‘˜è¦ ---
            content = (
                f"<summary_section index='{i+1}'>\n" # ç§»é™¤ id='S-x'ï¼Œæ”¹ç”¨ index
                f"  <content>{doc.page_content}</content>\n"
                f"</summary_section>"
            )
        else:
            # --- æ ¼å¼ B: åŸå§‹ç‰‡æ®µ ---
            # 1. è§£æ Metadata
            for key, value in meta.items():
                if isinstance(value, str) and (value.startswith("{") or value.startswith("[")):
                    try: meta[key] = json.loads(value)
                    except: pass
            
            # 2. æå–æ›´å¯è¯»çš„åœºæ™¯ä¿¡æ¯ï¼Œæ›¿ä»£å†·å†°å†°çš„ ID
            # å°è¯•è·å–åœºæ™¯åã€æ—¶é—´æˆ–åœ°ç‚¹ï¼Œç»„åˆæˆä¸€ä¸ª readable_source
            scene_id = meta.get('scene', 'Unknown_Scene')
            location = meta.get('loc', '') or meta.get('location', '')
            
            # æ„é€ ä¸€ä¸ªç»™ LLM çœ‹çš„â€œæ¥æºæ ‡ç­¾â€ï¼Œä¾‹å¦‚ï¼š[åœºæ™¯: story_main_... | åœ°ç‚¹: ç»ƒä¹ å®¤]
            # è¿™æ · LLM å°±ç®—å¼•ç”¨ï¼Œä¹Ÿä¼šå¼•ç”¨æˆ "åœ¨ç»ƒä¹ å®¤çš„åœºæ™¯ä¸­..."
            source_tag = f"Scene: {scene_id}"
            if location:
                source_tag += f", Location: {location}"

            meta_json = json.dumps(meta, ensure_ascii=False, indent=2)
            
            content = (
                f"<story_fragment sequence='{i+1}'>\n" # ç§»é™¤ id='F-x'
                f"  <source_info>{source_tag}</source_info>\n" # æ˜¾å¼å‘Šè¯‰ LLM è¿™æ˜¯ä»€ä¹ˆåœºæ™¯
                f"  <content>\n{doc.page_content}\n  </content>\n"
                f"  <metadata>\n{meta_json}\n  </metadata>\n"
                f"</story_fragment>"
            )
            
        formatted.append(content)
        
    return "\n\n".join(formatted)


def _dedupe_documents(docs):
    """
    é€šè¿‡æ–‡æ¡£é¡ºåº/point id å»é‡ï¼Œé¿å…å¤šè·¯æ£€ç´¢åé‡å¤çš„ä¸Šä¸‹æ–‡å¹²æ‰° rerankã€‚
    """
    seen = set()
    unique = []
    for doc in docs:
        meta = doc.metadata or {}
        key = None
        if meta.get("order") is not None:
            key = f"order:{meta.get('order')}"
        elif meta.get("id"):
            key = f"id:{meta.get('id')}"
        elif meta.get("scene"):
            key = f"scene:{meta.get('scene')}"
        else:
            key = doc.page_content[:120]
        if key in seen:
            continue
        seen.add(key)
        unique.append(doc)
    return unique


def _rerank_with_fallback(docs, query, reranker, limit):
    if not docs:
        return []
    try:
        reranked = reranker.compress_documents(docs, query=query)
        if reranked:
            return reranked[:limit]
    except Exception:
        pass
    return docs[:limit]


def _retrieve_detail_docs(raw_store, reranker, dense_query, sparse_query, raw_query, k_dense=180, k_sparse=120, limit=20, alpha=0.35):
    """
    åŒé€šé“ç»†èŠ‚æ£€ç´¢ï¼š
    - dense_query: é€‚é…è¯­ä¹‰é€šé“çš„æ—¥æ–‡å®Œæ•´é—®å¥ã€‚
    - sparse_query: å…³é”®è¯ä¸²ï¼Œåå‘ BM25ã€‚
    - raw_query: ç”¨æˆ·åŸæ–‡ï¼Œä¿ç•™ç¼–å·/æœªä¿®æ­£çš„å…³é”®è¯å…œåº•ã€‚
    """
    doc_pool = []
    dense_k = max(20, int(k_dense * (0.6 + alpha)))   # alpha è¶Šå¤§ï¼Œdense è¶Šå¤š
    sparse_k = max(20, int(k_sparse * (1.4 - alpha))) # alpha è¶Šå°ï¼Œsparse è¶Šå¤š
    print(f"ğŸ”§ [Internal] æ£€ç´¢å‚æ•°: dense_k={dense_k}, sparse_k={sparse_k}, alpha={alpha}")

    def _run(retriever, query):
        if hasattr(retriever, "get_relevant_documents"):
            return retriever.get_relevant_documents(query)
        if hasattr(retriever, "invoke"):
            return retriever.invoke(query)
        return []

    # è¯­ä¹‰ä¼˜å…ˆçš„ä¸»æ£€ç´¢
    semantic_retriever = raw_store.as_retriever(search_kwargs={"k": dense_k})
    doc_pool.extend(_run(semantic_retriever, dense_query))

    # ä¿ç•™å…³é”®è¯çš„å›è½æ£€ç´¢
    lexical_retriever = raw_store.as_retriever(search_kwargs={"k": sparse_k})
    doc_pool.extend(_run(lexical_retriever, sparse_query or raw_query))

    # ç”¨æˆ·åŸæ–‡å†å…œåº•ä¸€æ¬¡ï¼Œå…¼é¡¾åŸå§‹è¯­è¨€/ç¼–å·
    if raw_query and raw_query != sparse_query:
        doc_pool.extend(_run(lexical_retriever, raw_query))

    doc_pool = _dedupe_documents(doc_pool)
    return _rerank_with_fallback(doc_pool, dense_query or sparse_query or raw_query, reranker, limit)


def _parse_alpha(alpha_str: str, default: float = 0.35) -> float:
    try:
        val = float(alpha_str.strip())
        return max(0.15, min(0.65, val))
    except Exception:
        return default

# ==============================================================================
# ğŸŒ API æ¥å£ (ä¾› api_server.py è°ƒç”¨)
# ==============================================================================

_rag_components = None  # å…¨å±€ç¼“å­˜

def get_rag_components():
    """è·å–/åˆå§‹åŒ– RAG ç»„ä»¶ï¼ˆå•ä¾‹ï¼‰"""
    global _rag_components
    if _rag_components:
        return _rag_components
    
    print("ğŸ”§ åˆå§‹åŒ– RAG ç»„ä»¶...")
    
    client = QdrantClient(url=QDRANT_URL)
    dense_emb = XinferenceEmbeddings(server_url=XINFERENCE_URL, model_uid=EMBED_MODEL)
    sparse_emb = FastEmbedSparse(model_name="Qdrant/bm25")
    
    raw_store = QdrantVectorStore(
        client=client, collection_name=RAW_COLLECTION_NAME,
        embedding=dense_emb, sparse_embedding=sparse_emb,
        sparse_vector_name=SPARSE_VECTOR_NAME, retrieval_mode=RetrievalMode.HYBRID
    )
    
    summary_store = None
    if client.collection_exists(SUMMARY_COLLECTION_NAME):
        summary_store = QdrantVectorStore(
            client=client, collection_name=SUMMARY_COLLECTION_NAME,
            embedding=dense_emb, retrieval_mode=RetrievalMode.DENSE
        )
    
    llm = ChatOpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY, model=LLM_MODEL_NAME,
                     temperature=0.7, streaming=False, max_tokens=20480)
    rewrite_llm = ChatOpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY, model=LLM_MODEL_NAME,
                             temperature=0.0, streaming=False)
    
    reranker = XinferenceRerank(url=f"{XINFERENCE_URL.rstrip('/')}/v1/rerank",
                                model_uid=RERANK_MODEL, top_n=20, request_timeout=240)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=reranker, base_retriever=raw_store.as_retriever(search_kwargs={"k": 150})
    )

    intent_chain = ChatPromptTemplate.from_template(INTENT_TEMPLATE) | rewrite_llm | StrOutputParser()
    dense_rewrite_chain = ChatPromptTemplate.from_template(DENSE_REWRITE_TEMPLATE) | rewrite_llm | StrOutputParser()
    sparse_rewrite_chain = ChatPromptTemplate.from_template(SPARSE_KEYWORD_TEMPLATE) | rewrite_llm | StrOutputParser()
    alpha_chain = ChatPromptTemplate.from_template(ALPHA_TEMPLATE) | rewrite_llm | StrOutputParser()
    answer_chain = ChatPromptTemplate.from_template(ANSWER_TEMPLATE) | llm | StrOutputParser()
    
    _rag_components = {
        'summary_store': summary_store,
        'compression_retriever': compression_retriever,
        'raw_store': raw_store,
        'reranker': reranker,
        'rewrite_llm': rewrite_llm,
        'llm': llm,
        'intent_chain': intent_chain,
        'dense_rewrite_chain': dense_rewrite_chain,
        'sparse_rewrite_chain': sparse_rewrite_chain,
        'alpha_chain': alpha_chain,
        'answer_chain': answer_chain
    }
    print("âœ… RAG ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    return _rag_components


def process_single_query(user_query: str):
    """
    å¤„ç†å•ä¸ªæŸ¥è¯¢ï¼ˆä¾› API è°ƒç”¨ï¼Œæµå¼è¿”å›ï¼‰
    
    ä½¿ç”¨æ¨¡å—çº§å¸¸é‡ INTENT_TEMPLATE, DENSE_REWRITE_TEMPLATE, SPARSE_KEYWORD_TEMPLATE, ANSWER_TEMPLATE
    """
    c = get_rag_components()
    
    # æ‰§è¡ŒæŸ¥è¯¢æµç¨‹
    print(f"\nğŸ” [Internal] å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
    intent = c['intent_chain'].invoke({"query": user_query}).strip().lower()
    print(f"ğŸ’¡ [Internal] è¯†åˆ«æ„å›¾: {intent}")
    
    combined_docs = []
    dense_query = c['dense_rewrite_chain'].invoke({"question": user_query}).strip()
    sparse_query = c['sparse_rewrite_chain'].invoke({"question": user_query}).strip()
    alpha_raw = c['alpha_chain'].invoke({
        "original": user_query,
        "dense": dense_query,
        "sparse": sparse_query
    }).strip()
    alpha = _parse_alpha(alpha_raw, default=0.35)
    print(f"ğŸ”„ [Internal] è¯­ä¹‰é‡å†™ (JP): {dense_query}")
    print(f"ğŸ§© [Internal] å…³é”®è¯é‡å†™ (BM25): {sparse_query}")
    print(f"âš–ï¸ [Internal] Alpha å»ºè®®: {alpha_raw} -> é‡‡ç”¨ {alpha}")
    
    if 'analysis' in intent:
        if c['summary_store']:
            print("ğŸ“… [Internal] æ­£åœ¨æ£€ç´¢å®è§‚èƒŒæ™¯ (Summary)...")
            combined_docs.extend(c['summary_store'].similarity_search(user_query, k=5))
        
        print("ğŸ§ª [Internal] åŒé€šé“æ£€ç´¢ç»†èŠ‚ (JP rewrite + åŸæ–‡å…³é”®è¯)...")
        detail_docs = _retrieve_detail_docs(
            raw_store=c['raw_store'],
            reranker=c['reranker'],
            dense_query=dense_query,
            sparse_query=sparse_query,
            raw_query=user_query,
            k_dense=200,
            k_sparse=140,
            limit=18,
            alpha=alpha
        )
        combined_docs.extend(detail_docs)
        
    elif 'overview' in intent and c['summary_store']:
        print("ğŸ“– [Internal] æ­£åœ¨æ£€ç´¢å®è§‚æ‘˜è¦...")
        combined_docs = c['summary_store'].similarity_search(user_query, k=10)
        
    else:
        print("ğŸ•µï¸ [Internal] åŒé€šé“æ£€ç´¢äº‹å® (JP rewrite + åŸæ–‡å…³é”®è¯)...")
        combined_docs = _retrieve_detail_docs(
            raw_store=c['raw_store'],
            reranker=c['reranker'],
            dense_query=dense_query,
            sparse_query=sparse_query,
            raw_query=user_query,
            k_dense=220,
            k_sparse=160,
            limit=20,
            alpha=alpha
        )
    
    print(f"ğŸ“š [Internal] æ£€ç´¢å®Œæˆï¼Œå…±è·å– {len(combined_docs)} ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µ")
    
    if not combined_docs:
        yield "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        return
    
    context_str = format_docs(combined_docs)
    answer_chunks = []
    for chunk in c['answer_chain'].stream({"context": context_str, "original_question": user_query}):
        answer_chunks.append(chunk)
        yield chunk
    full_answer = "".join(answer_chunks)
    _maybe_trigger_onebot_tool(user_query, full_answer, combined_docs)


def main():
    print(f"\nå¯åŠ¨åˆ†å±‚æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (Hierarchical RAG)...")
    
    # è§¦å‘ç»„ä»¶åˆå§‹åŒ–ï¼Œå¤ç”¨ API åŒæ¬¾ç®¡çº¿
    get_rag_components()

    # --- äº¤äº’å¾ªç¯ ---
    while True:
        print("\n" + "="*50)
        user_query = input("ğŸ™‹ è¯·æé—® (ä¸­æ–‡) [qé€€å‡º]: ")
        if user_query.lower() in ['q', 'exit']: break
        
        try:
            for chunk in process_single_query(user_query):
                print(chunk, end="", flush=True)
            print()
            
        except Exception as e:
            print(f"\nâŒ æµç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
