import os
import json
import time
import warnings

# å±è”½çƒ¦äººçš„è­¦å‘Š
warnings.filterwarnings("ignore")

# --- æ ¸å¿ƒç»„ä»¶ ---
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# --- æ£€ç´¢ç»„ä»¶ ---
from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
from langchain_community.embeddings import XinferenceEmbeddings
from qdrant_client import QdrantClient

# --- è‡ªå®šä¹‰ç»„ä»¶ ---
try:
    from reranker import XinferenceRerank
except ImportError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° reranker.py")
    exit(1)

try:
    from langchain.retrievers import ContextualCompressionRetriever
except ImportError:
    from langchain_classic.retrievers import ContextualCompressionRetriever

# ==============================================================================
# ğŸ› ï¸ é…ç½®åŒºåŸŸ (è¯·ç¡®ä¿ä¸ build_hierarchy.py ä¸€è‡´)
# ==============================================================================

# 1. æ£€ç´¢åç«¯ (Xinference)
XINFERENCE_URL = os.getenv("XINFERENCE_SERVER_URL", "http://192.168.123.113:9997")
EMBED_MODEL = "bge-m3"
RERANK_MODEL = "bge-reranker-v2-m3"

# 2. å‘é‡æ•°æ®åº“ (Qdrant)
QDRANT_URL = os.getenv("QDRANT_URL", "http://qdrant:6333")
RAW_COLLECTION_NAME = "story_knowledge_base"  # ç»†èŠ‚ç¢ç‰‡
SUMMARY_COLLECTION_NAME = "story_summary_store" # å®è§‚æ‘˜è¦
SPARSE_VECTOR_NAME = "langchain-sparse"

# 3. ç”Ÿæˆåç«¯ (LLM)
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.example.com/v1")
LLM_API_KEY = os.getenv("LLM_API_KEY", "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "your-")

# ==============================================================================

def format_docs(docs):
    """
    æ™ºèƒ½æ ¼å¼åŒ–ï¼šæ··åˆå¤„ç† 'æ‘˜è¦(Summary)' å’Œ 'åŸå§‹ç‰‡æ®µ(Fragment)'
    [ä¼˜åŒ–]: ç§»é™¤å¯¹ LLM è¯±å¯¼æ€§å¼ºçš„ F-x IDï¼Œæ”¹ç”¨è¯­ä¹‰åŒ–æ ‡ç­¾
    """
    formatted = []
    for i, doc in enumerate(docs):
        meta = doc.metadata.copy()
        score = meta.pop("relevance_score", 0)
        
        # åˆ¤æ–­æ˜¯æ‘˜è¦è¿˜æ˜¯åŸå§‹ç‰‡æ®µ
        is_summary = meta.get("level") == "summary"
        
        if is_summary:
            # --- æ ¼å¼ A: æ‘˜è¦ ---
            content = (
                f"<summary_section index='{i+1}'>\n" # ç§»é™¤ id='S-x'ï¼Œæ”¹ç”¨ index
                f"  <content>{doc.page_content}</content>\n"
                f"</summary_section>"
            )
        else:
            # --- æ ¼å¼ B: åŸå§‹ç‰‡æ®µ ---
            # 1. è§£æ Metadata
            for key, value in meta.items():
                if isinstance(value, str) and (value.startswith("{") or value.startswith("[")):
                    try: meta[key] = json.loads(value)
                    except: pass
            
            # 2. æå–æ›´å¯è¯»çš„åœºæ™¯ä¿¡æ¯ï¼Œæ›¿ä»£å†·å†°å†°çš„ ID
            # å°è¯•è·å–åœºæ™¯åã€æ—¶é—´æˆ–åœ°ç‚¹ï¼Œç»„åˆæˆä¸€ä¸ª readable_source
            scene_id = meta.get('scene', 'Unknown_Scene')
            location = meta.get('loc', '') or meta.get('location', '')
            
            # æ„é€ ä¸€ä¸ªç»™ LLM çœ‹çš„â€œæ¥æºæ ‡ç­¾â€ï¼Œä¾‹å¦‚ï¼š[åœºæ™¯: story_main_... | åœ°ç‚¹: ç»ƒä¹ å®¤]
            # è¿™æ · LLM å°±ç®—å¼•ç”¨ï¼Œä¹Ÿä¼šå¼•ç”¨æˆ "åœ¨ç»ƒä¹ å®¤çš„åœºæ™¯ä¸­..."
            source_tag = f"Scene: {scene_id}"
            if location:
                source_tag += f", Location: {location}"

            meta_json = json.dumps(meta, ensure_ascii=False, indent=2)
            
            content = (
                f"<story_fragment sequence='{i+1}'>\n" # ç§»é™¤ id='F-x'
                f"  <source_info>{source_tag}</source_info>\n" # æ˜¾å¼å‘Šè¯‰ LLM è¿™æ˜¯ä»€ä¹ˆåœºæ™¯
                f"  <content>\n{doc.page_content}\n  </content>\n"
                f"  <metadata>\n{meta_json}\n  </metadata>\n"
                f"</story_fragment>"
            )
            
        formatted.append(content)
        
    return "\n\n".join(formatted)

def main():
    print(f"\nå¯åŠ¨åˆ†å±‚æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (Hierarchical RAG)...")
    
    # 1. åˆå§‹åŒ–è¿æ¥
    client = QdrantClient(url=QDRANT_URL)
    dense_emb = XinferenceEmbeddings(server_url=XINFERENCE_URL, model_uid=EMBED_MODEL)
    sparse_emb = FastEmbedSparse(model_name="Qdrant/bm25")
    
    # --- åˆå§‹åŒ–ä¸¤ä¸ª Vector Store ---
    raw_store = QdrantVectorStore(
        client=client,
        collection_name=RAW_COLLECTION_NAME,
        embedding=dense_emb,
        sparse_embedding=sparse_emb,
        sparse_vector_name=SPARSE_VECTOR_NAME,
        retrieval_mode=RetrievalMode.HYBRID # ç»†èŠ‚ï¼šHybrid Search
    )
    
    # æ£€æŸ¥æ‘˜è¦é›†åˆæ˜¯å¦å­˜åœ¨
    if not client.collection_exists(SUMMARY_COLLECTION_NAME):
        print(f"âš ï¸ æ³¨æ„ï¼šå®è§‚æ‘˜è¦é›†åˆ '{SUMMARY_COLLECTION_NAME}' ä¸å­˜åœ¨ï¼")
        print("    è¯·å…ˆè¿è¡Œ 'build_hierarchy.py' æ¥ç”Ÿæˆæ‘˜è¦ç´¢å¼•ã€‚")
        summary_store = None
    else:
        summary_store = QdrantVectorStore(
            client=client,
            collection_name=SUMMARY_COLLECTION_NAME,
            embedding=dense_emb,
            retrieval_mode=RetrievalMode.DENSE # æ‘˜è¦ï¼šDense Search å³å¯
        )


    # 2. åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        base_url=LLM_BASE_URL,
        api_key=LLM_API_KEY,
        model=LLM_MODEL_NAME,
        temperature=0.7, # ç”¨äºç”Ÿæˆå›ç­”
        streaming=True,
        max_tokens=10240
    )

    rewrite_llm = ChatOpenAI(
        base_url=LLM_BASE_URL,
        api_key=LLM_API_KEY,
        model=LLM_MODEL_NAME,
        temperature=0.0, # ç”¨äºç¿»è¯‘å’Œåˆ†ç±»ï¼Œä¿è¯ç¨³å®šæ€§
        streaming=False
    )
    
    # 1. æ„å›¾åˆ†ç±»é“¾ (Intent Classification Chain)
    intent_template = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ„å›¾åˆ†æä¸“å®¶ã€‚è¯·åˆ†æç”¨æˆ·çš„å…³äºã€Šè²ä¹‹ç©ºå¥³å­¦é™¢ã€‹å‰§æƒ…çš„æé—®ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{query}

ã€åˆ†ç±»å®šä¹‰ã€‘ï¼š
1. **analysis (æ·±åº¦åˆ†æ)**: 
   - è¯¢é—®è§’è‰²ä¹‹é—´çš„**å…³ç³»ã€æ„Ÿæƒ…ã€æ€åº¦**ï¼ˆå¦‚â€œåŸå­æ€ä¹ˆçœ‹èŠ±å¸†â€ã€â€œä¸¤äººçš„å…³ç³»å˜åŒ–â€ï¼‰ã€‚
   - è¯¢é—®**åŸå› ã€åŠ¨æœºã€èƒŒæ™¯**ï¼ˆå¦‚â€œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšâ€ã€â€œèƒŒåçš„å«ä¹‰â€ï¼‰ã€‚
   - è¯¢é—®**æ€§æ ¼ã€è¯„ä»·ã€æˆé•¿**ã€‚
   - **ç»å¤§å¤šæ•°éçº¯äº‹å®æ£€ç´¢çš„é—®é¢˜éƒ½åº”å½’ä¸ºæ­¤ç±»ã€‚**

2. **overview (å®è§‚æ¦‚æ‹¬)**: 
   - ä»…å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚â€œæ€»ç»“å…¨æ–‡â€ã€â€œæ¦‚æ‹¬æŸç« å¤§æ„â€ã€â€œè®²äº†ä»€ä¹ˆæ•…äº‹â€æ—¶ã€‚

3. **fact (äº‹å®è¿½é—®)**: 
   - è¯¢é—®æå…¶å…·ä½“çš„**æ—¶é—´ã€åœ°ç‚¹ã€æ¬¡æ•°ã€ç‰©å“**ï¼ˆå¦‚â€œç¬¬å‡ è¯å“­äº†â€ã€â€œåƒçš„ä»€ä¹ˆâ€ã€â€œIDæ˜¯å¤šå°‘â€ï¼‰ã€‚
   - å¯»æ‰¾å…·ä½“çš„æŸå¥å°è¯å‡ºå¤„ã€‚

è¯·åªè¾“å‡ºå…¶ä¸­ä¸€ä¸ªæ ‡ç­¾ï¼š'analysis', 'overview', æˆ– 'fact'ã€‚
"""
    intent_prompt = ChatPromptTemplate.from_template(intent_template)
    intent_chain = intent_prompt | rewrite_llm | StrOutputParser()

    # 2. ç»†èŠ‚æ£€ç´¢è·¯å¾„ (Specific Path)
    
    # æŸ¥è¯¢é‡å†™ Prompt (ä¸ä½ æä¾›çš„æœ€æ–°ç‰ˆä¸€è‡´)
    rewrite_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸º **RAG æ··åˆæ£€ç´¢ç³»ç»Ÿ (Hybrid Search)** è®¾è®¡çš„æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚... [æ­¤å¤„çœç•¥ï¼Œé€»è¾‘ä¸ä½ æä¾›çš„æœ€æ–°ç‰ˆæœ¬ä¸€è‡´]"""
    # ... (å°†æœ€æ–°çš„ rewrite_template æ”¾åˆ°è¿™é‡Œ)
    rewrite_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸º **RAG æ··åˆæ£€ç´¢ç³»ç»Ÿ (Hybrid Search)** è®¾è®¡çš„æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚
è¯¥ç³»ç»Ÿçš„ä¸‹æ¸¸åŒ…å«ä¸¤ä¸ªæ£€ç´¢å¼•æ“ï¼Œä½ éœ€è¦æ„é€ ä¸€ä¸ªèƒ½åŒæ—¶æ»¡è¶³å®ƒä»¬éœ€æ±‚çš„æ—¥æ–‡æŸ¥è¯¢è¯­å¥ï¼š

1. **è¯­ä¹‰æ£€ç´¢å¼•æ“ (Dense Vector / BGE-M3)**: 
   - åå¥½ï¼šå®Œæ•´çš„è‡ªç„¶è¯­è¨€å¥å­ï¼ŒåŒ…å«ä¸»è¯­ã€è°“è¯­ã€å®¾è¯­ã€‚
   - ç›®æ ‡ï¼šç†è§£â€œè°åšäº†ä»€ä¹ˆâ€ã€â€œæŸç§æ°›å›´â€ç­‰æŠ½è±¡è¯­ä¹‰ã€‚
   - è¦æ±‚ï¼š**ä¸è¦**ç ´åå¥å­çš„è¯­æ³•ç»“æ„ã€‚

2. **å…³é”®è¯æ£€ç´¢å¼•æ“ (Sparse Vector / BM25)**:
   - åå¥½ï¼šç²¾ç¡®çš„ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€æ›²åï¼‰å’Œç¨€æœ‰è¯æ±‡ã€‚
   - ç›®æ ‡ï¼šé€šè¿‡å­—é¢åŒ¹é…é€šè¿‡ IDF æœºåˆ¶è¿‡æ»¤æ— å…³æ–‡æ¡£ã€‚
   - è¦æ±‚ï¼šå¿…é¡»åŒ…å«å‡†ç¡®çš„è§’è‰²åï¼ˆå¦‚â€œä¹™å®—æ¢¢â€è€Œéâ€œæ¢¢â€ï¼‰ã€‚

3. **æœ¯è¯­ä¿®æ­£ (Term Correction)**: ç”¨æˆ·å¯èƒ½ä¼šä½¿ç”¨é”™è¯¯çš„æ±‰å­—æˆ–ä¸å‡†ç¡®çš„ç§°å‘¼ã€‚è¯·åˆ©ç”¨ä½ çš„çŸ¥è¯†åº“è¿›è¡Œä¿®æ­£ã€‚
    - ç¤ºä¾‹: "ä¸‰å‰‘å£«" -> åº”è¯¥ä¿®æ­£ä¸º "ä¸‰éŠƒå£«" (å› ä¸ºè²ä¹‹ç©ºå‰§æƒ…ä¸­å¸¸ç”¨çš„æ˜¯â€œãŠæ°—æ¥½ä¸‰éŠƒå£«â€)
    ã€è§’è‰²åæ˜ å°„è¡¨ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)ã€‘ï¼š
    - æ¢¢ / æ¢¢å‰è¾ˆ -> ä¹™å®—æ¢¢
    - èŠ±å¸† / èŠ±å¸†æ¡‘ -> æ—¥é‡ä¸‹èŠ±å¸†
    - æ…ˆ / æ…ˆå‰è¾ˆ -> è—¤å³¶æ…ˆ
    - ç‘ ç’ƒä¹ƒ -> å¤§æ²¢ç‘ ç’ƒä¹ƒ
    - åŸå­ / ç™¾ç”Ÿ / å°åŸå­ -> ç™¾ç”ŸåŸå­
    - å°é“ƒ / å¾’ç”º -> å¾’ç”ºå°éˆ´
    - å§¬èŠ½ / å®‰é¤Šå¯º -> å®‰é¤Šå¯ºå§«èŠ½
    - å¡æ‹‰æ–¯ -> ã‚»ãƒ©ã‚¹ (æˆ–æ˜µç§° ã‚»ã£ã¡ã‚ƒã‚“)

ã€æ‰§è¡Œæ­¥éª¤ã€‘ï¼š
1. **æ„å›¾ç†è§£**: ...
2. **è‡ªç„¶ç¿»è¯‘**: ...
3. **åŒä¹‰è¯æ³›åŒ– (å…³é”®)**: 
   - é’ˆå¯¹åŠ¨ä½œæˆ–æƒ…æ„Ÿæè¿°ï¼Œ**å¿…é¡»**æ‰©å±•ç›¸å…³çš„æ—¥æ–‡åŒä¹‰è¯ï¼Œä»¥ç¡®ä¿å…³é”®è¯æ£€ç´¢ (BM25) èƒ½å‘½ä¸­ä¸åŒæè¿°é£æ ¼çš„ç‰‡æ®µã€‚
   - ç¤ºä¾‹: "å“­" -> å¿…é¡»åŒ…å« `æ³£ã æ¶™ å·æ³£ æ¶™å£° æ³£ãé¡”`
   - ç¤ºä¾‹: "ç¬‘" -> å¿…é¡»åŒ…å« `ç¬‘ã† ç¬‘é¡” å¾®ç¬‘ã‚€`
4. **è¯é¢˜é”å®š**: ...

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
[ä¿®æ­£åçš„è‡ªç„¶æ—¥æ–‡é—®å¥] [åŒä¹‰è¯æ‰©å±•] [ç±»å‹å…³é”®è¯] [æ ¸å¿ƒè¯é¢˜è¯]

ã€ç¤ºä¾‹ã€‘ï¼š
ç”¨æˆ·: æ¢¢å“­äº†å‡ æ¬¡ï¼Ÿ
è¾“å‡º: ä¹™å®—æ¢¢ã¯ä½•å›æ³£ãã¾ã—ãŸã‹ï¼Ÿ æ³£ã æ¶™ å·æ³£ æ¶™å£° æ³£ãé¡” ã‚·ãƒ¼ãƒ³ (åŒ…å«æ‰€æœ‰"å“­"ç›¸å…³çš„è¯)

ç”¨æˆ·: è°åœ¨ç»ƒä¹ å®¤ç”Ÿæ°”äº†ï¼Ÿ
è¾“å‡º: ç·´ç¿’å®¤ã§èª°ãŒæ€’ã‚Šã¾ã—ãŸã‹ï¼Ÿ æ€’ã‚‹ æ¿€æ€’ ä¸æ©Ÿå«Œ å–§å˜© å ´æ‰€

ç”¨æˆ·é—®é¢˜: {question}
"""
    rewrite_prompt = ChatPromptTemplate.from_template(rewrite_template)
    # é‡å†™é“¾ä½¿ç”¨æ¸©åº¦ä¸º 0 çš„ LLM
    rewrite_chain = rewrite_prompt | rewrite_llm | StrOutputParser()
    
    # Reranker é…ç½®
    reranker = XinferenceRerank(
        url=f"{XINFERENCE_URL.rstrip('/')}/v1/rerank",
        model_uid=RERANK_MODEL,
        top_n=25,
        request_timeout=240
    )
    
    # ç»†èŠ‚æ£€ç´¢å™¨
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=reranker,
        base_retriever=raw_store.as_retriever(search_kwargs={"k": 150})
    )

    # ç»†èŠ‚ç”Ÿæˆ Prompt
    specific_answer_template = """ä½ æ˜¯ä¸€ä¸ªç²¾é€šã€Šè²ä¹‹ç©ºå¥³å­¦é™¢ã€‹å‰§æƒ…çš„ AI å‰§æƒ…åˆ†æå¸ˆã€‚
è¯·é˜…è¯»ä»¥ä¸‹å‰§æƒ…ç‰‡æ®µï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å‰§æƒ…ç‰‡æ®µã€‘ï¼š
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{original_question}

ã€å›ç­”è¦æ±‚ã€‘ï¼š
1. **å®Œå…¨èåˆå¼å†™ä½œ**ï¼šè¯·å°†æ£€ç´¢åˆ°çš„å‰§æƒ…ç»†èŠ‚**èåˆ**åœ¨ä½ çš„åˆ†æä¸­ï¼Œ**ç»å¯¹ä¸è¦**å‡ºç° "F-1", "F-2", "ç‰‡æ®µX", "story_main_xxx" è¿™æ ·çš„æœºæ¢°ç´¢å¼•æˆ–æ–‡ä»¶åã€‚
2. **è‡ªç„¶å¼•ç”¨**ï¼š
   - âŒ é”™è¯¯ç¤ºèŒƒï¼šæ ¹æ® F-6ï¼Œç‘ ç’ƒè¯´å¤§å®¶å˜äº†ã€‚
   - âœ… æ­£ç¡®ç¤ºèŒƒï¼šåœ¨ä¿±ä¹éƒ¨å‘ç”Ÿå˜åŒ–çš„æ—¶æœŸï¼ˆstory_main_10500701_scene_005ï¼‰ï¼Œç‘ ç’ƒæ›¾å¯¹å§¬èŠ½è¡¨ç¤ºâ€œå¤§å®¶éƒ½ä¸åœ¨äº†ï¼Œæ„Ÿè§‰éƒ½å˜äº†â€ï¼Œè¿™ä½“ç°äº†...
   - âœ… æ­£ç¡®ç¤ºèŒƒï¼šå½“ä¸¤äººåœ¨é’“é±¼åœºç‹¬å¤„æ—¶ï¼ˆåœºæ™¯ï¼šé’“é±¼ï¼‰ï¼Œå§¬èŠ½æåˆ°äº†...
3. **å±•ç¤ºè¯æ®**ï¼šæ—¢ç„¶ç”¨æˆ·çœ‹ä¸åˆ°åŸæ–‡ï¼Œä½ å¿…é¡»**å¤è¿°**å…³é”®å°è¯æˆ–åŠ¨ä½œæå†™ä½œä¸ºè¯æ®ï¼Œè€Œä¸æ˜¯åªç»™ä¸€ä¸ªç»“è®ºã€‚
4. **å¦‚æœæœªæ‰¾åˆ°**ï¼šç›´æ¥è¯´â€œåœ¨å½“å‰æ£€ç´¢åˆ°çš„å‰§æƒ…ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚
"""
    specific_answer_prompt = ChatPromptTemplate.from_template(specific_answer_template)

    # èåˆç”Ÿæˆ Prompt
    fusion_answer_template = """ä½ æ˜¯ä¸€ä¸ªç²¾é€šã€Šè²ä¹‹ç©ºå¥³å­¦é™¢ã€‹å‰§æƒ…çš„ä¸“å®¶ã€‚
ä¸ºäº†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºä½ æä¾›äº†ä¸¤ä¸ªå±‚é¢çš„ä¿¡æ¯ï¼š
1. **<summary_context>**: å‰§æƒ…çš„å®è§‚æ‘˜è¦ã€‚
2. **<evidence_fragment>**: å…·ä½“çš„å¯¹è¯å’Œåœºæ™¯ç»†èŠ‚ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
{original_question}

ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼š
{context}

ã€å›ç­”è¦æ±‚ã€‘ï¼š
1. **æ·±åº¦åˆ†æä¸èåˆ**ï¼šå…ˆç”¨å®è§‚æ‘˜è¦å®šä¸‹åŸºè°ƒï¼Œå†**ç›´æ¥å¼•ç”¨**å…·ä½“ç‰‡æ®µä¸­çš„å°è¯æˆ–åŠ¨ä½œç»†èŠ‚ä½œä¸ºè®ºæ®ã€‚
2. **éšå½¢å¼•ç”¨**ï¼š
   - **ä¸¥ç¦**ä½¿ç”¨ "F-X", "S-X", "ID:xxx" è¿™ç§è®¡ç®—æœºæœ¯è¯­ã€‚
   - è¯·ä½¿ç”¨â€œåœ¨...åœºæ™¯ä¸­â€ã€â€œå½“...çš„æ—¶å€™â€ã€â€œæ­£å¦‚...æ‰€è¯´â€è¿™ç§è‡ªç„¶çš„å™è¿°æ–¹å¼æ¥å¼•ç”¨æ¥æºã€‚
   - **å¿…é¡»å¤è¿°å†…å®¹**ï¼šä¸è¦è¯´â€œå¦‚ F-1 æ‰€è¿°â€ï¼Œè¦è¯´â€œæ­£å¦‚ä¹‹å‰æåˆ°çš„å§¬èŠ½å¤±å»æŒšå‹çš„ç»å†...â€ã€‚
3. **ç»“æ„åŒ–å›ç­”**ï¼šå¯ä»¥åˆ†ç‚¹ä½œç­”ï¼Œä½†æ¯ä¸€å¤„çš„è®ºæ®å¿…é¡»æ˜¯å…·ä½“çš„å‰§æƒ…æè¿°ï¼Œè€Œéç¼–å·ã€‚
4. å¿…é¡»ç”¨**ä¸­æ–‡**å›ç­”ã€‚
"""
    fusion_answer_prompt = ChatPromptTemplate.from_template(fusion_answer_template)

    # --- äº¤äº’å¾ªç¯ ---
    while True:
        print("\n" + "="*50)
        user_query = input("ğŸ™‹ è¯·æé—® (ä¸­æ–‡) [qé€€å‡º]: ")
        if user_query.lower() in ['q', 'exit']: break
        
        try:
            # 1. æ„å›¾åˆ†ç±»
            print(f"ğŸ¤– æ­£åœ¨åˆ†ææ„å›¾...", end="", flush=True)
            intent = intent_chain.invoke({"query": user_query}).strip().lower()
            print(f"\râœ… æ„å›¾è¯†åˆ«: ã€{intent}ã€‘           ")
            
            combined_docs = []

            # =================================================
            # ğŸš€ ç­–ç•¥ A: æ·±åº¦åˆ†æ (Analysis) -> åŒè·¯æ£€ç´¢èåˆ
            # =================================================
            if 'analysis' in intent:
                print("ğŸ”„ å¯åŠ¨åŒè·¯æ£€ç´¢ (Dual-Path Retrieval)...")
                
                # Path 1: æŸ¥æ‘˜è¦ (è·å–èƒŒæ™¯)
                if summary_store:
                    print("   â””â”€â”€ æ­£åœ¨æå–å®è§‚èƒŒæ™¯ (Summary)...")
                    # æ‘˜è¦ä¸éœ€è¦é‡å†™ï¼Œç›´æ¥ç”¨ä¸­æ–‡æœè¯­ä¹‰å³å¯ï¼Œå– Top 5
                    summary_docs = summary_store.similarity_search(user_query, k=5)
                    combined_docs.extend(summary_docs)
                
                # Path 2: æŸ¥ç»†èŠ‚ (è·å–è¯æ®)
                print("   â””â”€â”€ æ­£åœ¨æŒ–æ˜ç»†èŠ‚è¯æ® (Details)...")
                jp_query = rewrite_chain.invoke({"question": user_query})
                # ç»†èŠ‚æ£€ç´¢éœ€è¦é‡å†™ä¸ºæ—¥æ–‡
                detail_docs = compression_retriever.invoke(jp_query)
                # æˆ‘ä»¬å–å‰ 15 ä¸ªç»†èŠ‚ï¼Œé¿å…å†²æ·¡æ‘˜è¦çš„æƒé‡
                combined_docs.extend(detail_docs[:15])

            # =================================================
            # ğŸ“– ç­–ç•¥ B: å®è§‚æ¦‚æ‹¬ (Overview) -> åªæŸ¥æ‘˜è¦
            # =================================================
            elif 'overview' in intent and summary_store:
                print("ğŸ” æ£€ç´¢å®è§‚æ‘˜è¦...")
                combined_docs = summary_store.similarity_search(user_query, k=10)

            # =================================================
            # ğŸ” ç­–ç•¥ C: äº‹å®è¿½é—® (Fact) -> åªæŸ¥ç»†èŠ‚
            # =================================================
            else: # fact æˆ– fallback
                print("ğŸ” æ£€ç´¢å…·ä½“ç»†èŠ‚...")
                jp_query = rewrite_chain.invoke({"question": user_query})
                combined_docs = compression_retriever.invoke(jp_query)

            # --- ç»Ÿä¸€ç”Ÿæˆç¯èŠ‚ ---
            if not combined_docs:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
                continue

            # æ ¼å¼åŒ–æ‰€æœ‰æ–‡æ¡£ï¼ˆè‡ªåŠ¨å¤„ç†æ··åˆç±»å‹ï¼‰
            context_str = format_docs(combined_docs)
            
            print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆæ·±åº¦å›ç­” (Context Size: {len(combined_docs)} chunks)...")
            print("-" * 30)
            
            # ä½¿ç”¨èåˆ Prompt
            final_chain = fusion_answer_prompt | llm | StrOutputParser()
            
            for chunk in final_chain.stream({
                "context": context_str,
                "original_question": user_query
            }):
                print(chunk, end="", flush=True)
            print("\n")
            
        except Exception as e:
            print(f"\nâŒ æµç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()